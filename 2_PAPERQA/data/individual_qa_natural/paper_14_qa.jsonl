{"question": "In the Paper ID 14, What is at the forefront of current research in explainable machine learning?", "answer": "Computationally efficient estimation of post-hoc explanations is at the forefront of current research in explainable machine learning."}
{"question": "In the Paper ID 14, Which researchers are cited regarding computationally efficient estimation of post-hoc explanations?", "answer": "Strumbelj & Kononenko (2010), Slack et al. (2021), Jethani et al. (2022), Chen et al. (2023), Donnelly et al. (2023), Muschalik et al. (2024) are cited."}
{"question": "In the Paper ID 14, What aspect do most works focus on to improve efficiency in explanation estimation?", "answer": "Most works focus on improving efficiency with respect to the dimension of features, specific model classes like neural networks and decision trees, or approximating the conditional feature distribution."}
{"question": "In the Paper ID 14, Which papers focus on feature dimension efficiency?", "answer": "Covert et al. (2020), Jethani et al. (2022), Chen et al. (2023), Fumagalli et al. (2023) focus on feature dimension efficiency."}
{"question": "In the Paper ID 14, Which model classes are targeted for efficiency improvements in explanation estimation?", "answer": "Neural networks and decision trees are targeted for efficiency improvements."}
{"question": "In the Paper ID 14, Which papers address efficiency for neural networks in explanation estimation?", "answer": "Erion et al. (2021) and Chen et al. (2024) address efficiency for neural networks."}
{"question": "In the Paper ID 14, Which papers address efficiency for decision trees in explanation estimation?", "answer": "Muschalik et al. (2024) addresses efficiency for decision trees."}
{"question": "In the Paper ID 14, What is typically used in practical settings to estimate explanations?", "answer": "A marginal feature distribution is typically used to estimate explanations in practical settings."}
{"question": "In the Paper ID 14, What are background data samples also known as?", "answer": "Background data samples are also known as reference points or baselines."}
{"question": "In the Paper ID 14, Why are background data samples important in explanation estimation?", "answer": "They play a crucial role in the estimation process by providing reference points for explanation methods."}
{"question": "In the Paper ID 14, Which paper mentions using draws from the marginal distribution for SAGE sampling approximation?", "answer": "Covert et al. (2020) mentions using draws from the marginal distribution for SAGE sampling approximation."}
{"question": "In the Paper ID 14, How many fixed background samples did Covert et al. (2020) use for SAGE?", "answer": "They used a fixed set of 512 background samples."}
{"question": "In the Paper ID 14, What motivates the research question in the paper?", "answer": "References in literature and practical usage motivate the question: Can we reliably improve on standard i.i.d. sampling in explanation estimation?"}
{"question": "In the Paper ID 14, What statistical theory concept is connected to explanation estimation in this paper?", "answer": "Kernel thinning (kt) is connected to explanation estimation."}
{"question": "In the Paper ID 14, Who introduced kernel thinning?", "answer": "Dwivedi & Mackey introduced kernel thinning in 2021 and 2022."}
{"question": "In the Paper ID 14, What is the purpose of kernel thinning?", "answer": "Kernel thinning is used to compress a distribution more effectively than i.i.d. sampling."}
{"question": "In the Paper ID 14, Which algorithm implements kernel thinning efficiently?", "answer": "The compress++ algorithm (Shetty et al., 2022) implements kernel thinning efficiently."}
{"question": "In the Paper ID 14, In what context was kernel thinning previously applied?", "answer": "Kernel thinning was applied to improve statistical kernel testing."}
{"question": "In the Paper ID 14, What does the paper aim to quantify regarding the sample then explain paradigm?", "answer": "The paper aims to quantify the error introduced by the sample then explain paradigm in feature marginalization."}
{"question": "In the Paper ID 14, What types of explanations are involved in feature marginalization estimation?", "answer": "Both local and global removal-based explanations are involved."}
{"question": "In the Paper ID 14, What is the proposed efficient way to reduce approximation error in explanation estimation?", "answer": "The paper proposes reducing approximation error based on distribution compression."}
{"question": "In the Paper ID 14, What is the first main contribution of the paper?", "answer": "Quantifying the error of standard i.i.d. sampling in explanation estimation."}
{"question": "In the Paper ID 14, What can the error from i.i.d. sampling lead to?", "answer": "It can lead to changes in feature importance rankings."}
{"question": "In the Paper ID 14, What is the second contribution of the paper?", "answer": "Introduction of the 'compress then explain' (cte) paradigm: a sample-efficient explainability approach."}
{"question": "In the Paper ID 14, What justifies the cte paradigm theoretically?", "answer": "A discovered connection between explanation estimation and distribution compression justifies the cte paradigm."}
{"question": "In the Paper ID 14, What is the third contribution stated in the paper?", "answer": "Empirical demonstration that kernel thinning (kt) outperforms i.i.d. sampling in compressing distributions for explainable machine learning datasets."}
{"question": "In the Paper ID 14, Is this the first work to evaluate kernel thinning for supervised learning datasets?", "answer": "Yes, this is the first work to evaluate distribution compression via kernel thinning on supervised learning datasets."}
{"question": "In the Paper ID 14, What is the fourth contribution of the paper?", "answer": "Decreasing the computational cost of explanation estimation."}
{"question": "In the Paper ID 14, How does cte affect the accuracy and variance of explanations?", "answer": "cte results in more accurate explanations with smaller variance."}
{"question": "In the Paper ID 14, How many fewer samples does cte require to achieve similar error as standard methods?", "answer": "cte often achieves on-par error using 2–3× fewer samples."}
{"question": "In the Paper ID 14, How does cte impact the number of model inferences required?", "answer": "cte requires 2–3× fewer model inferences."}
{"question": "In the Paper ID 14, Is cte compatible with various explanation methods?", "answer": "Yes, cte is a simple plug-in for a broad class of methods that sample from a dataset."}
{"question": "In the Paper ID 14, What types of explanation methods can benefit from cte?", "answer": "Removal-based and global explanations, among others, can benefit from cte."}
{"question": "In the Paper ID 14, What role does the marginal distribution play in explanation estimation?", "answer": "The marginal distribution is used to sample background data for estimating explanations."}
{"question": "In the Paper ID 14, What is a common method for obtaining background samples in explanation estimation?", "answer": "Standard i.i.d. sampling from the dataset is a common method."}
{"question": "In the Paper ID 14, What is the main limitation of standard i.i.d. sampling identified in the paper?", "answer": "Standard i.i.d. sampling introduces approximation error in explanation estimation."}
{"question": "In the Paper ID 14, What is the goal of distribution compression in the context of explanations?", "answer": "To reduce the approximation error and improve sample efficiency in explanation estimation."}
{"question": "In the Paper ID 14, How does kernel thinning compare to i.i.d. sampling in compressing distributions?", "answer": "Kernel thinning outperforms i.i.d. sampling in compressing distributions, leading to better estimation."}
{"question": "In the Paper ID 14, What is the sample then explain paradigm?", "answer": "It is the practice of drawing samples from the data and then estimating explanations based on those samples."}
{"question": "In the Paper ID 14, What are removal-based explanations?", "answer": "Removal-based explanations are explanation methods that estimate the effect of removing features from a model."}
