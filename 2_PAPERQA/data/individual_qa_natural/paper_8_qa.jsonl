{"question": "In the Paper ID 8, What are some of the tasks involved in image editing?", "answer": "Tasks in image editing include adding or removing objects, changing colors, textures, or styles, and taking actions such as moving objects or changing actor positions."}
{"question": "In the Paper ID 8, Why is image editing considered a complex task?", "answer": "Image editing is complex because it requires fine-grained understanding of visual scene composition and reasoning, such as spatial instructions and referring expressions."}
{"question": "In the Paper ID 8, What is the 'inpainting paradigm' in image editing models?", "answer": "The 'inpainting paradigm' refers to models that perform localized changes, typically adding or removing objects or editing attributes within an image."}
{"question": "In the Paper ID 8, What are the limitations of current image editing models?", "answer": "Current models can only perform localized edits like object addition/removal or attribute changes and cannot handle edits requiring holistic visual understanding or complex actions."}
{"question": "In the Paper ID 8, How have some researchers tried to address the limitations of image editing models?", "answer": "Researchers have introduced specialized model architectures to handle different image editing subtasks, but these still lack holistic visual understanding."}
{"question": "In the Paper ID 8, What types of edits require more holistic visual understanding?", "answer": "Edits such as 'make the cook cut the apple in half' or 'make the dog jump in the air' require holistic visual understanding and reasoning about interactions and events."}
{"question": "In the Paper ID 8, Are action-centric edits well-studied in instruction-tuned image editing models?", "answer": "No, action-centric edits are severely understudied in instruction-tuned image editing models."}
{"question": "In the Paper ID 8, How are action-centric edits typically considered in existing research?", "answer": "When considered, action-centric edits are studied in isolation, ignoring other subtasks and lacking rigorous semantic evaluation."}
{"question": "In the Paper ID 8, What is a major reason for the limitations in image editing model capabilities?", "answer": "The lack of high-quality data for training models is a major reason for these limitations."}
{"question": "In the Paper ID 8, Why is finetuning data for object or attribute changes easier to acquire?", "answer": "Such data is easier to get because inpainting setups leverage strong object and attribute abilities of text-to-image models for paired-image data generation."}
{"question": "In the Paper ID 8, Why is solving data scarcity for action and reasoning-centric edits more challenging?", "answer": "It is more challenging because generating high-quality paired examples for these types of edits requires more complex data sources and minimal changes."}
{"question": "In the Paper ID 8, What two sources are identified as promising for action and reasoning-centric edit data?", "answer": "Videos and simulation engines are identified as the most promising data sources for these edit types."}
{"question": "In the Paper ID 8, What happens when models are trained on noisy synthetic image pairs or video frames?", "answer": "Models trained on noisy synthetic data tend to have poor editing abilities."}
{"question": "In the Paper ID 8, What does 'noisy' mean in the context of synthetic image pairs?", "answer": "'Noisy' refers to image pairs with changes not mentioned in the prompt, often due to automatic generation shortcomings or irrelevant changes in video frames."}
{"question": "In the Paper ID 8, What is the main requirement for high-quality action and reasoning-centric edit examples?", "answer": "High-quality examples must be truly minimal, containing only one or two semantic changes described by the prompt, with all other aspects unchanged."}
{"question": "In the Paper ID 8, What is the AURORA Dataset?", "answer": "The AURORA Dataset is a curated collection of truly-minimal image edit pairs from videos and simulation engines for instruction-tuned image editing."}
{"question": "In the Paper ID 8, How many examples from videos and simulation engines are collected in AURORA?", "answer": "AURORA contains 130K examples from videos and 150K from simulation engines."}
{"question": "In the Paper ID 8, How is the AURORA dataset collected?", "answer": "It is collected via crowd-sourcing and curation from diverse video sources and simulation engines."}
{"question": "In the Paper ID 8, What do commonly used image-text-alignment metrics measure in image editing?", "answer": "These metrics mostly measure visual similarity to groundtruth and the ability to stay faithful (i.e., copy) to the source image."}
{"question": "In the Paper ID 8, Why are existing metrics insufficient for action and reasoning-centric edits?", "answer": "They have almost no correlation with a modelâ€™s ability to generate accurate action and reasoning-centric edits."}
{"question": "In the Paper ID 8, What is AURORA-BENCH?", "answer": "AURORA-BENCH is a manually annotated benchmark for evaluating image editing models on 8 editing tasks using human judgment."}
{"question": "In the Paper ID 8, What inspires the novel discriminative metric introduced in the paper?", "answer": "It is inspired by work on using image generation models as discriminators."}
{"question": "In the Paper ID 8, What does the novel discriminative metric assess?", "answer": "It assesses both understanding and hallucination in image editing models."}
{"question": "In the Paper ID 8, How is the efficacy of AURORA demonstrated in the paper?", "answer": "By presenting a state-of-the-art instruction-tuned image editing model finetuned on AURORA and evaluated on AURORA-BENCH, compared to strong baselines."}
{"question": "In the Paper ID 8, What are the four main contributions summarized in the paper?", "answer": "1) Creation of AURORA dataset, 2) Comprehensive benchmark, 3) Novel metric, 4) State-of-the-art image editing model with well-rounded capabilities."}
{"question": "In the Paper ID 8, What types of edit abilities does the new image editing model cover?", "answer": "It covers object-centric, action-centric, and reasoning-centric edit abilities."}
{"question": "In the Paper ID 8, Why is faithfulness an important first step for image editing models?", "answer": "Faithfulness ensures the edited image remains as close as possible to the source image except for the intended changes."}
{"question": "In the Paper ID 8, What is the importance of semantic evaluation in image editing?", "answer": "Semantic evaluation is crucial to assess the accuracy and relevance of edits beyond mere visual similarity."}
{"question": "In the Paper ID 8, What is meant by 'instruction-tuned' image editing?", "answer": "Instruction-tuned image editing refers to models trained to perform edits based on specific textual instructions."}
{"question": "In the Paper ID 8, How are paired-image data generated for object or attribute changes?", "answer": "Paired-image data are generated by leveraging text-to-image models in inpainting setups."}
{"question": "In the Paper ID 8, What is a challenge with using video frames for edit training data?", "answer": "Video frames often introduce unwanted changes like viewpoint shifts or non-meaningful movement, resulting in noisy training data."}
{"question": "In the Paper ID 8, Which section of the paper describes the typology of edit types and dataset limitations?", "answer": "Section 2 describes the typology of edit types and how existing datasets fail to address them all."}
{"question": "In the Paper ID 8, Which section details the AURORA dataset and collection process?", "answer": "Section 3 describes the AURORA dataset and its collection process."}
{"question": "In the Paper ID 8, What does Section 4 introduce?", "answer": "Section 4 introduces AURORA-BENCH, a manually annotated benchmark for image editing tasks."}
{"question": "In the Paper ID 8, What is covered in Section 5.1?", "answer": "Section 5.1 describes the novel discriminative metric for assessing understanding and hallucination."}
{"question": "In the Paper ID 8, What is discussed in Section 5.3?", "answer": "Section 5.3 presents experimental comparisons of the AURORA-trained model against strong baselines."}
{"question": "In the Paper ID 8, What does the action-reasoning-object-attribute acronym (AURORA) signify?", "answer": "It signifies the inclusion of action-centric, reasoning-centric, object-centric, and attribute-centric edit examples in the dataset."}
{"question": "In the Paper ID 8, Why is human judgement used in AURORA-BENCH?", "answer": "Human judgment provides a more reliable evaluation of edit accuracy than automated metrics."}
{"question": "In the Paper ID 8, How does the paper claim its model advances the field of image editing?", "answer": "By presenting a well-rounded model trained on diverse, high-quality data, evaluated with more informative metrics and benchmarks."}
{"question": "In the Paper ID 8, What are the shortcomings of automatic generation processes in image editing datasets?", "answer": "Automatic generation often introduces changes not mentioned in prompts, leading to noisy and less useful training data."}
