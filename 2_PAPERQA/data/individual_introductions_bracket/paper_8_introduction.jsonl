{"text": "[Paper ID: 8]\n\nImage editing is a complex task, involving many different skills, from adding/removing objects, changing colors/textures/styles, to “taking actions”: moving objects, changing actor positions or even more complex interactions. Tackling all of these requires fine-grained understanding of how visual scenes are composed as well as reasoning (e.g. spatial instructions or referring expressions). No current model can successfully do all of these edits, and most only perform localized changes involving object addition/removal or attribute edits, following the “inpainting paradigm” [Zhang et al., 2024, Xie et al., 2023]. Others have tried to address this issue by introducing more specialized model architectures which handle different editing subtasks [Couairon et al., 2023, Zhang et al., 2023a]. However, neither of these approaches includes edits requiring more holistic visual understanding of how humans and objects interact or how events unfold, such as ‘make the cook cut the apple in half’ or ‘make the dog jump in the air’ (see Fig. 1). These more action-centric edits are severely understudied in the space of instruction-tuned image editing models [Brooks et al., 2023, Huang et al., 2024]; when they are considered, it is done in isolation, ignoring other image edit subtasks and rigorous semantic evaluation [Soucek et al., 2023, Black et al., 2024]. In Sec. 2 we ˇdescribe a typology of these edit types and how existing datasets currently fail to address them all.\n        \n        As we argue in this paper, a major reason for these limitations is the lack of high-quality data. Finetuning data of object or attribute changes is simpler to acquire than other forms of edits, since inpainting setups directly leverage strong object and attribute abilities of txt2img models [Rombach et al., 2022] for paired-image data generation [Yildirim et al., 2023, Zhang et al., 2024]. However, solving the data scarcity for learning action and reasoning-centric edits is not as straightforward. We identify videos and simulation engines as the two most promising sources of data for these edit types. As we discuss in this paper, we find that previous models trained on “noisy” synthetic image pairs or video frames lead to poor editing abilities. Here, noisy refers to image pairs with changes not mentioned in the prompt, i.e. due to shortcomings of the automatic generation process or inherent properties of videos such as viewpoint changes and non-meaningful movement. Therefore, our main requirement of high-quality action and reasoning-centric edit examples is that they be truly minimal: Edited images which contain one or maximally two semantic changes described by the prompt, while all other aspects are kept exactly the same. From a diverse set of video sources and simulation engines, we curate the AURORA Dataset (Action-Reasoning-Object-Attribute). Via crowd-sourcing and curation we collect 130K truly-minimal examples from videos and 150K from simulation engines for instruction-tuned image editing. We describe our dataset and collection process in Sec. 3.\n        \n        The few image-text-alignment metrics commonly used in image editing are based on visual similarity to a groundtruth and in reality turn out to mostly measure the ability to stay maximally faithful (i.e. copying) to the source image [Zhang et al., 2024, Fu et al., 2023]. Though faithfulness is an important first step to master, these metrics have almost no correlation with the model’s ability to generate accurate edits, especially on action and reasoning-centric changes. Hence, in addition to the training data in AURORA, we introduce AURORA-BENCH(Sec. 4), a manually annotated benchmark covering 8 editing tasks on which we collect human judgement (Tab. 2). Inspired by work on image generation models as discriminators [Krojer et al., 2023, Li et al., 2023], we also describe a novel discriminative metric that assesses understanding and hallucination (Sec. 5.1). To demonstrate the efficacy and quality of AURORA, we present a state-of-the-art instruction-tuned image editing model, finetuned on AURORA and evaluated on AURORA-BENCH, which we compare to strong baselines in a set of experiments in Sec. 5.3.\n        \n        In summary our contributions are: 1) The creation of AURORA, a new clean and varied set of image edit pairs for instruction-finetuning that encompasses more action-centric and reasoningcentric examples. 2) We present a comprehensive benchmark covering a variety of edit types; 3) We introduce a novel more informative metric beyond existing ones; 4) We provide a state-of-the-art image editing model based on AURORA with well-rounded image editing capabilities covering object-centric, action-centric, and reasoning-centric edit abilities."}
