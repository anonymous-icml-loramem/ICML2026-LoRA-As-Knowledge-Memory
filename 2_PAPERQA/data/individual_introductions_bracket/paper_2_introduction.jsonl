{"text": "[Paper ID: 2]\n\nSelf-attention-based models, such as transformers (Vaswani et al., 2017), exhibit a remarkable ability known as in-context learning (Brown et al., 2020). That is, these models can solve unseen tasks based on exemplars in the context of an input prompt. In-context learning (ICL) is critical to the flexibility of large language models, allowing them to solve tasks not explicitly included in their training data. However, it remains unclear how architectures like self-attention acquire this ability through gradient descent training.\n\nSeminal work by Olsson et al. (2022) identified an intriguing trait in the training dynamics of ICL: the ICL ability often emerges abruptly, coinciding with an abrupt drop in loss during training. This abrupt learning phase can reflect the formation of an induction head in the ICL setting (Olsson et al., 2022; Reddy, 2024; Singh et al., 2024; Edelman et al., 2024), and can also occur more broadly in transformer training dynamics (Nanda et al., 2023; Chen et al., 2024a; Hoffmann et al., 2024). Furthermore, Singh et al. (2023) found that ICL may often be a transient ability that the transformers acquire and then lose over the course of long training time, a phenomenon that has since been reproduced in many settings (He et al., 2024; Anand et al., 2025; Chan et al., 2025; Nguyen & Reddy, 2025; Park et al., 2025; Singh et al., 2025). These findings underscore the importance of understanding not only the ICL ability in trained models, but its full training dynamics.\n\nThis work aims to provide a theoretical description of how the ICL ability evolves in gradient descent training. To do so, we consider the increasingly common setup of linear attention1 (Von Oswald et al., 2023) trained on an in-context linear regression task (Garg et al., 2022). The in-context linear regression task, in which the model needs to perform linear regression on the data in context, is a canonical instantiation of ICL (Garg et al., 2022; Akyürek et al., 2023; Von Oswald et al., 2023; Ahn et al., 2023; Bai et al., 2023). The linear attention model, which has been used in many prior studies (Schlag et al., 2021; Von Oswald et al., 2023; Ahn et al., 2023; Zhang et al., 2024a; Wu et al., 2024; Fu et al., 2024; Mahankali et al., 2024; Duraisamy, 2024; Li et al., 2024; Yau et al., 2024; Lu et al., 2024; Frei & Vardi, 2025), reproduces key optimization properties of practical transformers (Ahn et al., 2024) and is more amenable to theoretical analysis. Importantly, despite its name, linear attention is a nonlinear model, as it removes the softmax operation but is still a nonlinear function of the input.\n\nWe study two common parametrizations of multi-head linear attention: (i) ATTN_M, linear attention where the key and query matrices in each head are merged into a single matrix, a reparametrization procedure widely used in theoretical studies on transformers (Ahn et al., 2023; Tian et al., 2023; Ataee Tarzanagh et al., 2023; Zhang et al., 2024a, b; Chen et al., 2024b; Wu et al., 2024; Kim & Suzuki, 2024; Huang et al., 2024b; Wang et al., 2024b; Ildiz et al., 2024; Ren et al., 2024; Tarzanagh et al., 2024; Vasudeva et al., 2025; Lu et al., 2024; Chen & Li, 2024; Julistiono et al., 2024; Yau et al., 2024; Anwar et al., 2024; Huang et al., 2025a); (ii) ATTN_s, linear attention with separate key and query matrices, which is closer to the implementation of attention in real-world transformers (Vaswani et al., 2017). We specify the fixed points in the loss landscapes, as well as how gradient descent training dynamics traverses the landscape. Our findings are summarized as follows.\n\n• We find two fixed points in the training dynamics of ATTN_M, and exponentially many fixed points in that of ATTN_S.\n\n• We show a single, abrupt loss drop in training ATTN_M from small initialization and derive an analytical time-course solution when the input token covariance is white. We show saddle-to-saddle training dynamics in training ATTN_S\n from small initialization and reduce the high-dimensional training dynamics to scalar ordinary differential equations through an ansatz. We demonstrate the rank of the separate key and query weights affects the dynamics by shortening the duration of certain plateaus.\n\n• We identify the in-context algorithm of the converged and early stopped models. When ATTN_M and ATTN_S are trained to convergence, they approximately implement least squares linear regression in context. When the training of ATTN_S early stops during the (m+1)-th loss plateau, it approximately implements principal component regression in context with the first m principal components.\n\n• As a tool for our analysis, we show that when trained on in-context linear regression tasks, ATTN_M is equivalent to a two-layer fully-connected linear network with a cubic feature map as input, and ATTN_S is equivalent to a sum of three-layer convolutional linear networks with the same cubic feature map as input.\n\n• We empirically demonstrate that the single and multiple loss drops also occur in softmax ATTN_M and ATTN_S, respectively.\n\nComparing the two models, we find that the ICL ability evolves differently in them: ATTN_M acquires the in-context linear regression ability through one abrupt loss drop, while ATTN_S acquires this ability by progressively improving on in-context principal component regression. This makes a theoretical case for the progressive improvements of ICL in gradient descent training. Our results also reveal how parametrization, such as merged versus separate key and query and the rank of the separate key and query weights, influences the loss landscape and training dynamics. This motivates future research to take the parametrization factor into account when studying the landscape and dynamics of attention models."}
