{"text": "[Paper ID: 11]\n\nDespite the nonconvex nature of neural networks, training them with local gradient methods finds nearly optimal parameters. Understanding the properties of the loss landscape is theoretically important, as it enables us to depict the learning dynamics of neural networks. For instance, many existing works prove that the loss landscape is “benign” in some sense - i.e. they don’t have spurious local minima, bad valleys, or decreasing path to infinity Kawaguchi (2016), Venturi et al. (2019), Haeffele & Vidal (2017), Sun et al. (2020), Wang et al. (2021b), Liang et al. (2022). Such characterization enlightens our intuition on why these networks are trained so well.\n\nAs part of understanding the loss landscape, understanding the structure of global optimum has gained much interest. An example is mode connectivity Garipov et al. (2018), where a simple curve connects two global optima in the set of optimal parameters. Another example is analyzing the permutation symmetry that a global optimum has Simsek et al. (2021). Mathematically understanding the global optimum is important as it sheds light on the structure of the loss landscape. They can also motivate practical algorithms that search over neural networks with the same optimal cost Ainsworth et al. (2022), Mishkin & Pilanci (2023), having practical motivations to study. \n\nWe shape the loss landscape of regularized neural networks with ReLU activation, mainly analyzing mathematical properties of the global optimum, by considering its convex counterpart and leveraging the dual problem. Our work is inspired by the work of Mishkin & Pilanci (2023), where they characterize the optimal set and stationary points of a two-layer neural network with weight decay using the convex counterpart. They also introduce several important concepts such as the polytope characterization of the optimal solution set, minimal solutions, pruning a solution, and the optimal model fit. Expanding the idea of Mishkin & Pilanci (2023), we show a clear connection between the polytope characterization and the dual optimum. We further derive novel characters of the optimal set of neural networks, the loss landscape, and generalize the result to different architectures.\n\nFinally, it is worth pointing out that regularization plays a central role in modern machine learning, including the training of large language models Andriushchenko et al. (2023). Therefore, including regularization better reflects the training procedure in practice.\n\nMore importantly, adding regularization can change the qualitative behavior of the loss landscape and the global optimum Wang et al. (2021b): for example, there always exist infinitely many optimal solutions for the unregularized problem with ReLU activation due to positive homogeneity. However, regularizing the parameter weights breaks this tie and we may not have infinitely many optimal solutions. It is also possible to design the regularization for the loss landscape to satisfy certain properties such as no spurious local minima Liang et al. (2022), Ge et al. (2017) or unique global optimum Mishkin & Pilanci (2023), Boursier & Flammarion (2023). Understanding the loss landscape of regularized neural networks is not only a more realistic setup but can also give novel theoretical properties that the unregularized problem does not have.\n\nThe specific findings we have for regularized neural networks are:\n\n• The optimal polytope: We revisit the fact that the regularized neural network’s convex reformulation has a polytope as an optimal set Mishkin & Pilanci (2023). We give a connection between the dual optimum and the polytope.\n\n• The staircase of connectivity: For two-layer neural networks with scalar output, we give critical widths and phase transitional behavior of the optimal set as the width of the network m changes. See Figure 1 for an abstract depiction of this phenomenon.\n\n• Nonunique minimum-norm interpolators: We examine the problem in Boursier & Flammarion (2023) and show that free skip connections (i.e., an unregularized linear neuron), bias in the training problem, and unidimensional data are all necessary to guarantee the uniqueness of the minimumnorm interpolator. We construct explicit examples where the solution is not unique in each case, inspired by the dual problem. In contrast to the previous perspectives Boursier & Flammarion (2023), Joshi et al. (2023), our results imply that free skip connections may change the qualitative behavior of optimal solutions. Moreover, uniqueness does not hold in dimensions greater than one.\n\n• Generalizations: We extend our results by providing a general description of solution sets of the cone-constrained group LASSO. The extensions include the existence of fixed first-layer weight directions for parallel deep neural networks, and connectivity of optimal sets for vector-valued neural networks with regularization.\n\nThe paper is organized as follows: after discussing related work (Section 1.1) and notations (Section 1.2), we discuss the convex reformulation of neural networks as a preliminary in Section 2. Then we discuss the case of two-layer neural networks with scalar output in Section 3, starting from the optimal polytope characterization (Section 3.1), the staircase of connectivity (Section 3.2), and construction of non-unique minimum-norm interpolators (Section 3.3). The possible generalizations are introduced in Section 4. Finally, we conclude the paper in Section 5. Detailed explanations of the experiments and proofs are deferred to the appendix."}
