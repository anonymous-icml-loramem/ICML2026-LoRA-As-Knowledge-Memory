{"text": "[Paper ID: 13]\n\nA striking feature of large language models (LLMs) is their ability to process high-level concepts through rich representations in their activations. This feature has given rise to techniques like activation steering (Turner et al., 2023), which leverage these learned representations to efficiently and predictably alter LLM behavior (Wang et al., 2024b; Zou et al., 2023; Rimsky et al., 2024).\n\nProblem: Lack of conditional control in activation steering. Activation steering offers a promising alternative to optimization-based techniques by directly manipulating the model’s native representations, often requiring only a simple activation addition step during each forward call (Turner et al., 2023). While activation steering has shown promise in altering LLM behavior, such as removing or inducing refusal behavior, a key limitation of current methods is the inability to condition when and what to refuse (Zheng et al., 2024; Ghandeharioun et al., 2024). That is, adding a “refusal vector” using existing activation steering methods increases refusal rates indiscriminately across all inputs, limiting the model’s utility (Arditi et al., 2024).\n\nContribution: Adding “control” to activation steering. We introduce Conditional Activation Steering (CAST), a method that enables fine-grained, context-dependent control over LLM behaviors. We introduce a new type of steering vector in the activation steering formulation, the condition vector, representing certain activation patterns induced by the prompt during the inference process. A simple similarity calculation between this condition vector and the model’s activation at inference time effectively serves as a switch, determining whether to apply the refusal vector. This approach allows for selective refusal of harmful prompts while maintaining the ability to respond to harmless ones, as depicted in Figure 1. A breakdown of this figure is presented in Table 3. Furthermore, CAST maintains the data, runtime, and compute efficiency of activation steering (Figure 6) while adding controllability, enabling the implementation of behavioral rules in LLMs without significant costs.\n\nApplication: Selecting what to refuse.   Many alignment goals concern contextually refusing specific classes of instructions (Anwar et al., 2024). Traditional methods like preference modeling are resource-intensive and struggle with subjective, black-box rewards (Feng et al., 2024; Pitis, 2023; Rafailov et al., 2024; Stiennon et al., 2020; Hayum et al.,). Additionally, the definition of harmful content varies across contexts (He et al., 2024b; Sorensen et al., 2024; Santurkar et al., 2023), complicating the creation of universal harm models. The usage context further complicates this variability; for instance, discussing medical advice might be harmful in some situations (Wang et al., 2023b) but essential in others, such as in medical chatbots (Xie et al., 2024a). In this paper, we show CAST can implement behavioral rules like “if input is about hate speech or adult content, then refuse” (Figure 8a) or “if input is not about legal advice, then refuse” (Figure 9a), allowing for selective modification of responses to specific content without weight optimization.\n\nOn a technical level, our primary insight is that different prompts consistently activate distinct patterns in the model’s hidden states during inference (Hu et al., 2024). These patterns can be extracted as a steering vector and used as reference points for detecting specific prompt categories or contexts. This observation allows us to use steering vectors not only as behavior modification mechanisms but also as condition indicators, which we term “condition vectors.” Our specific contributions are as follows:\n\n1) Framework: We introduce conditional activation steering and condition vectors, which adds a new dimension of controllability to existing methods.\n\n2) Application: We demonstrate the logical composition of condition vectors to create custom refusal conditions. This is a key step towards tailoring model behavior to specific needs.\n\n3) Codebase: We release a general-purpose activation steering toolkit with demo datasets for the broader activation engineering community <placeholder: open-source GitHub link>."}
