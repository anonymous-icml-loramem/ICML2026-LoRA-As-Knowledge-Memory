{"text": "This paper 7 discusses:\n\nScaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence (Kaplan et al., 2020; Brown et al., 2020; OpenAI, 2023; Team et al., 2023). However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers (Brown et al., 2020; Wenzek et al., 2019) to select training documents. These techniques significantly improve data quality and boost model performance.\n\nHowever, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in Figure 2 (Upper). Removing such tokens might alter the text’s meaning, while overly strict filtering could exclude useful data (Welbl et al., 2021; Muennighoff et al., 2024) and lead to biases (Dodge et al., 2021; Longpre et al., 2023). Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications (Tay et al., 2022; Wettig et al., 2023). For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can lead to inefficient computation on non-essential tokens, potentially restricting LLMs from achieving more advanced levels of intelligence.\n\nTo explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In §2.1, we evaluated the model’s token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are “easy tokens” that are already learned, and some are “hard tokens” that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.\n\nBased on these analyses, we introduce Rho-1 models trained with a novel Selective Language Modeling (SLM) objective 1. As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss (§2.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (§2.2).\n\nWe show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens. §3.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B Rho-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, Rho-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Upon fine-tuning, Rho-1-1B and 7B achieve 40.6% and 51.8% on MATH, respectively. Notably, Rho-1-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4’s CoT performance of 42.5%. §3.3 confirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks. In §3.4, we demonstrate that in settings without high-quality reference data, we can use SLM for self-referencing, leading to an average improvement of up to 3.3% in downstream tasks."}
