{"text": "This paper 14 discusses:\n\nComputationally efficient estimation of post-hoc explanations is at the forefront of current research on explainable machine learning (Strumbelj & Kononenko, 2010; Slack et al., 2021; Jethani et al., 2022; Chen et al., 2023; Donnelly et al., 2023; Muschalik et al., 2024). The majority of the work focuses on improving efficiency with respect to the dimension of features (Covert et al., 2020; Jethani et al., 2022; Chen et al., 2023; Fumagalli et al., 2023), specific model classes like neural networks (Erion et al., 2021; Chen et al., 2024) and decision trees (Muschalik et al., 2024), or approximating the conditional feature distribution (Chen et al., 2018; Aas et al., 2021; Olsen et al., 2022; 2024).\n\nHowever, in many practical settings, a marginal feature distribution is used instead to estimate explanations, and i.i.d. samples from the data typically form the so-called background data samples, also known as reference points or baselines, which plays a crucial role in the estimation process (Lundberg & Lee, 2017; Scholbeck et al., 2020; Erion et al., 2021; Ghalebikesabi et al., 2021; Lundstrom et al., 2022). For example, Covert et al. (2020) mention “[O]ur sampling approximation for SAGE was run using draws from the marginal distribution. We used a fixed set of 512 background samples […]” and we provide more such references in Appendix A to motivate our research question: Can we reliably improve on standard i.i.d. sampling in explanation estimation?\n\nWe make a connection to research on statistical theory, where kernel thinning (kt, Dwivedi & Mackey, 2021; 2022) was introduced to compress a distribution more effectively than with i.i.d. sampling. kt has an efficient implementation in the compress++ algorithm (Shetty et al., 2022) and was applied to improve statistical kernel testing (Domingo-Enrich et al., 2023). Building on this line of work, this paper aims to thoroughly quantify the error introduced by the current sample then explain paradigm in feature marginalization, which is involved in the estimation of both local and global removal-based explanations (Covert et al., 2021). We propose an efficient way to reduce this approximation error based on distribution compression (Figure 1).\n\nContribution. In summary, our work advances literature in multiple ways: (1) Quantifying the error of standard i.i.d. sampling: We bring to attention and measure the approximation error introduced by using i.i.d. sampling of background and foreground data in various explanation methods. It may even lead to changes in feature importance rankings. (2) Compress then explain (cte): We introduce a new paradigm of sample-efficient explainability where post-hoc explanations, like feature attributions and effects, are estimated based on a marginal distribution compressed more efficiently than with i.i.d. sampling. cte is theoretically justified as we discover a connection between explanation estimation and distribution compression. (3) Kernel thinning for (explainable) machine learning: We show empirically that kt outperforms i.i.d. sampling in compressing the distribution of popular datasets used in research on explainable machine learning. In fact, this is the first work to evaluate distribution compression via kt on datasets for supervised learning, which itself is valuable. (4) Decreasing the computational cost of explanation estimation: We benchmark compress then explain (cte) with popular explanation methods and show it results in more accurate explanations of smaller variance. cte often achieves on-par error using 2–3× fewer samples, i.e. requiring 2–3× fewer model inferences. cte is a simple, yet powerful, plug-in for a broad class of methods that sample from a dataset, e.g. removal-based and global explanations."}
