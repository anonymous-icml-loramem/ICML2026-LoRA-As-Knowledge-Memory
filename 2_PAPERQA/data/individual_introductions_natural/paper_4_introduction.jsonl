{"text": "This paper 4 discusses:\n\nIn practical data-driven decision-making tasks, integrating various types of data analysis steps is crucial for addressing diverse challenges. For instance, in genetic research aimed at identifying genes linked to a specific disease, the process often begins with preprocessing tasks such as filling in missing values and detecting outliers. This is followed by screening for potentially related genes using simple descriptive statistics and then applying more complex machine learning-based feature selection algorithms. Such a systematic sequence of steps designed to analyze data and derive useful insights is known as a data analysis pipeline, which plays a key role in ensuring the reproducibility and reliability of data-driven decision-making.\n\nIn this study, as an example of data analysis pipelines, we consider a class of feature selection pipelines that integrates various missing-value imputations (MVI) algorithms, outlier detection (OD) algorithms, and feature selection (FS) algorithms. Figure 1 shows examples of two such pipelines. The pipeline on the left starts with a mean value imputation algorithm, followed by L1 regression based OD algorithm, proceeds with marginal screening to refine feature candidates, and concludes by using two FS algorithms—stepwise feature selection and Lasso—selecting their union as the final features. The pipeline on the right initiates with regression imputation, continues with marginal screening to narrow down feature candidates, uses Cook’s distance for OD, and applies both stepwise FS and Lasso, ultimately choosing the intersection of their results as the final features.\n\nWhen a data-driven approach is used for high-stakes decision-making tasks such as medical diagnosis, it is crucial to quantify the reliability of the final results by considering all steps in the pipeline. The goal of this study is to develop a statistical test for a specific class of feature selection pipelines, allowing the statistical significance of features obtained through the pipeline to be properly quantified in the form of p-values. The first technical challenge in achieving this is the need to appropriately account for the complex interrelations between pipeline components to determine the overall statistical significance. The second challenge is to develop a universal framework capable of performing statistical tests on arbitrary pipelines (within a given class) rather than creating individual tests for each pipeline.\n\nTo address these challenges, we introduce the concept of selective inference (SI) (Taylor and Tibshirani, 2015; Fithian et al., 2015; Lee and Taylor, 2014), a novel statistical inference approach that has gained significant attention over the past decade. The core idea of SI is to characterize the process of selecting hypotheses from the data and calculate the corresponding p-values using the sampling distribution, conditional on this selection process. We propose an approach based on SI that provides valid p-values for any feature selection pipeline configuration within the aforementioned class. We also introduce a modular implementation framework that supports SI for any pipeline configuration within this class without requiring additional implementation efforts1. Specifically, with our framework, the statistical significance of features from any pipeline in this class can be quantified as valid p-values when used in a linear model, with no extra implementation required beyond specifying the pipeline.\n\nWe note that our long-term goal beyond this current study is to ensure the reproducibility of data-driven decision-making by accounting for the entire pipeline from raw data to the final results, with the current study on a class of feature selection pipelines serving as a proof of concept for that goal."}
