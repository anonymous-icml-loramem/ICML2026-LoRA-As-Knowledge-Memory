{"text": "This paper 9 discusses:\n\nReinforcement Learning (RL, Sutton and Barto, 2018) is the problem of learning how to interact with a changing environment. The setting usually consists of two major elements: a transition kernel, which governs how the state of the environment evolves due to the actions of an agent, and a reward given to the agent for performing an action at a given environment state. Agents must decide which actions to perform in order to collect as much reward as possible, taking into account not only the immediate reward gain, but also the long-term effects of actions on the state dynamics.\n\nIn the standard RL framework, reward information is usually observed after playing an action, and agents only aim to maximize their cumulative expected reward, also known as the value (Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018; Dann et al., 2019; Zanette and Brunskill, 2019; Efroni et al., 2019b; Simchowitz and Jamieson, 2019; Zhang et al., 2021b). Yet, in many real-world scenarios, partial information about the future reward is accessible in advance. For example, when performing transactions, prices are usually known. In navigation settings, rewards are sometimes associated with traffic, which can be accurately estimated for the near future. In goal-oriented problems (Schaul et al., 2015; Andrychowicz et al., 2017), the location of the goal is oftentimes revealed in advance. This information is completely ignored by agents that maximize the expected reward, even though using this future information on the reward should greatly increase the reward collected by the agent.\n\nAs an illustration, consider a driving problem where an agent travels between two locations, aiming to collect as much reward as possible. In one such scenario, rewards are given only when traveling free roads. It would then be reasonable to assume that agents see whether there is traffic before deciding in which way to turn at every intersection (‘one-step lookahead’). In an alternative scenario, the agent participates in ride-sharing and gains a reward when picking up a passenger. In this case, agents gain information on nearby passengers along the path, not necessarily just in the closest intersection (‘multi-step lookahead’). Finally, the destination might be revealed only at the beginning of the interaction, and reward is only gained when reaching it (‘full lookahead’). In all examples, the additional information should be utilized by the agent to increase its collected reward.\n\nIn this paper, we analyze the value of future (lookahead) information on the reward that could be obtained by the agent through the lens of competitive analysis. More precisely, we study the competitive ratio (CR) between the value of an agent that only has access to reward distributions and that of a lookahead agent who sees the actual reward realizations for several future timesteps before choosing each action. Our contributions are the following: (i) Given an environment and its expected rewards, we characterize the distribution that maximizes the value of lookahead agents, for all ranges of lookahead from one step to full lookahead; this distribution therefore minimizes the CR. In particular, we show that the lookahead value is maximized by long-shot rewards – very high rewards at extremely low probabilities. (ii) We derive the worst-case CR as a function of the dynamics of the environment (that is, for the worst-case reward expectations). Surprisingly, the CR that emerges is closely related to fundamental quantities in reward-free exploration and offline RL (Xie et al., 2022; Al-Marjani et al., 2023). (iii) We analyze the CR for the worst-possible environment. Specifically, tree-like environments that require deciding both when and where to navigate exhibit near-worst-case CR. (iv) Lastly, we complement these results by presenting different environments and their CR, providing more intuition to our results."}
