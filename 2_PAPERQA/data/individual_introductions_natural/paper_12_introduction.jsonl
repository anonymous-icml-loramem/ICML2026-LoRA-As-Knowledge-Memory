{"text": "This paper 12 discusses:\n\nFoundation models have revolutionized our world, demonstrating remarkable capabilities in solving grade school math problems, writing creative essays, generating stunning images, and comprehending visual content (openai2023gpt4; ChatGPT; ramesh2022hierarchical). One notable example is CLIP (radford2021learning), a vision-language model pretrained on a vast dataset of image-text pairs, which forms the backbone of numerous other foundation models (ramesh2022hierarchical; liu2023visualinstructiontuning). CLIP has achieved unprecedented performance across a wide range of benchmarks spanning many domains—a sharp contrast to models from the ImageNet era, which struggled to generalize from a training domain mostly consisting of natural photographs to stylistically different domains such as ImageNet-Sketch (wang2019learning), ImageNet-R (hendrycks2020many), and DomainNet (peng2019moment).\n\nDomains, while often challenging to quantify in practice (bendavid), emerge from collecting data from specific sources and conditions. Some domains, like natural images or renditions, are better delineated, allowing the creation of datasets like the ones mentioned above. Out-of-domain (OOD) generalization refers to a model’s ability to perform well on data from domains other than its training domain(s) (wang2022generalizingunseendomainssurvey). In this work, we collectively refer to the domain represented by ImageNet-Sketch, ImageNet-R, DomainNet-Painting, DomainNet-Clipart, DomainNet-Sketch, and DomainNet-Quickdraw as the rendition domain, since it contains images that are renditions of natural objects and scenes. Generalization to the rendition domain (especially OOD) is crucial for aligning models with human perception, as humans can interpret abstract visual renditions, while machines tend to rely heavily on textural cues (hendrycks2020many; geirhos2018imagenet).\n\nCLIP’s strong performance in several domains, including renditions, is attributed to its vast training distribution, rather than its contrastive learning objective, language supervision, or dataset size (fang2022data). However, fang2022data do not specify what characteristics of the training distribution drive this performance. CLIP could be learning more robust representations due to the diversity of natural images in its training set—or it may simply have been exposed to many datapoints from the (assumed to be OOD) test domains during training. Indeed, mayilvahanan2024 revealed that CLIP’s training data contains exact or near duplicates of samples of many OOD datasets. Yet, they showed that CLIP still generalizes well when this sample contamination is corrected. However, their analysis failed to account for domain contamination.\n\nIn contrast to sample contamination, domain contamination does not focus on duplicates of specific datapoints but rather examines whether critical aspects of a test domain are present in the training domain, such as images with different content but similar style to test samples. For example, after the correction by mayilvahanan2024, many other rendition images, while not duplicates, remained in CLIP’s training set (refer to Tab. LABEL:tab:domain_composition). Prior works often assume that CLIP is capable of generalizing OOD (radford2021learning; abbasi2024decipheringrolerepresentationdisentanglement; nguyen2024saftoutofdistributiongeneralizationfinetuning; fang2022data; li2023distillinglargevisionlanguagemodel; shu2023clipoodgeneralizingclipoutofdistributions); however, it remains unclear whether this is truly the case or if its performance is primarily driven by training on images from the test domain. This leads us to our central question:\n\nTo what extent does domain contamination explain CLIP’s performance on renditions?\n\nWe address the central question with the following contributions:\n\n• Constructing Clean Single-Domain Datasets: To rigorously test whether CLIP’s success in the rendition domain stems from their exposure during training, we first train a domain classifier to distinguish natural images from renditions (Sec. 3.2). By applying the domain classifier to a deduplicated version of LAION-400M, we create and release two datasets: LAION-Natural contains 57M natural images; LAION-Rendition consists of 16M renditions of scenes and objects. Additionally, we refine existing rendition OOD benchmarks (ImageNet-R, ImageNet-Sketch, etc.) by removing samples that do not belong to the corresponding domain (LABEL:sec:subsampling_datasets).\n\n• Refining the Evaluation of CLIP’s OOD Performance: Using LAION-Natural, we demonstrate that CLIP trained only on natural images significantly underperforms on rendition domain shifts (LABEL:sec:laion_nat_v_random). This suggests that its original success stems from domain contamination, not from an intrinsic OOD generalization ability (see Fig. 1 for a summary).\n\n• Investigating Domain Mixing and Scaling Effects: Our single-domain datasets enable analyzing the effects of training on controlled mixtures of natural and rendition images across scales (LABEL:sec:laion_mix). We identify the optimal mixing ratio for the best overall performance and show the degree to which training on one domain enables some generalization to the other.\n\nThrough this work, we aim to shed light on the limitations of foundation models like CLIP in handling OOD generalization and provide valuable datasets and tools to the community for further exploration. Fig. 1 illustrates our core methodology."}
