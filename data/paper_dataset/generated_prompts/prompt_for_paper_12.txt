You are an expert academic assistant tasked with creating a high-quality question-answering dataset from a research paper's introduction. Your goal is to generate 30 question-and-answer pairs based exclusively on the provided text.

Instructions and Rules:
Source Grounding: All questions and answers MUST be derived solely from the provided introduction text. Do not use any external knowledge or make assumptions beyond what is written.

Question Hierarchy: You must create questions across three distinct levels of understanding, as defined below.

Quantity: Generate exactly 30 pairs in total: 10 for Level 1, 10 for Level 2, and 10 for Level 3.

Output Format: The output must be a single, valid JSON array of objects. Do not include any explanatory text, comments, or markdown formatting before or after the JSON code block.

Question Level Definitions:
Level 1: Key Information Recall (10 Questions)

Objective: To test the recall of specific, explicitly stated facts, proper nouns, terminology, and figures from the text.

Question Type: "What is...?", "What are the names of...?", "Which X was mentioned...?"

Level 2: Contextual Comprehension (10 Questions)

Objective: To test the understanding of relationships between concepts, such as cause-and-effect, problem-solution, comparisons, and the function of a component.

Question Type: "Why does...?", "What is the effect of A on B?", "How does X work?", "What is the difference between A and B?"

Level 3: Logical Structure Inference (10 Questions)

Objective: To test the understanding of the overall logical flow of the text, including identifying the core problem, the research gap, the proposed solution, and the main contribution.

Question Type: "What is the core problem the authors aim to solve?", "What research gap does this paper intend to fill?", "What is the main advantage of the proposed method?", "Summarize the key contribution of this work in relation to prior limitations."

Desired JSON Output Format:
The final output MUST be a JSON array containing 30 objects. Each object must have three keys: level (integer: 1, 2, or 3), question (string), and answer (string).

Example:

[
  {
    "level": 1,
    "question": "What is the full name of the algorithm the authors integrate their estimator into?",
    "answer": "The authors integrated their estimator into the Generalized Successive Elimination algorithm."
  },
  {
    "level": 2,
    "question": "According to the text, what is the inverse relationship between response time and preference strength?",
    "answer": "The text states that users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicate weaker preferences."
  },
  {
    "level": 3,
    "question": "What is the core reason complex psychological models are impractical for real-time systems, and how does this paper's proposal address it?",
    "answer": "They are impractical because they rely on computationally intensive methods like hierarchical Bayesian inference and MLE. This paper addresses it by proposing a computationally efficient method that frames utility estimation as a linear regression problem."
  }
]

Introduction Text to Analyze:
Title: In Search of Forgotten Domain Generalization

Foundation models have revolutionized our world, demonstrating remarkable capabilities in solving grade school math problems, writing creative essays, generating stunning images, and comprehending visual content (openai2023gpt4; ChatGPT; ramesh2022hierarchical). One notable example is CLIP (radford2021learning), a vision-language model pretrained on a vast dataset of image-text pairs, which forms the backbone of numerous other foundation models (ramesh2022hierarchical; liu2023visualinstructiontuning). CLIP has achieved unprecedented performance across a wide range of benchmarks spanning many domains—a sharp contrast to models from the ImageNet era, which struggled to generalize from a training domain mostly consisting of natural photographs to stylistically different domains such as ImageNet-Sketch (wang2019learning), ImageNet-R (hendrycks2020many), and DomainNet (peng2019moment).

Domains, while often challenging to quantify in practice (bendavid), emerge from collecting data from specific sources and conditions. Some domains, like natural images or renditions, are better delineated, allowing the creation of datasets like the ones mentioned above. Out-of-domain (OOD) generalization refers to a model’s ability to perform well on data from domains other than its training domain(s) (wang2022generalizingunseendomainssurvey). In this work, we collectively refer to the domain represented by ImageNet-Sketch, ImageNet-R, DomainNet-Painting, DomainNet-Clipart, DomainNet-Sketch, and DomainNet-Quickdraw as the rendition domain, since it contains images that are renditions of natural objects and scenes. Generalization to the rendition domain (especially OOD) is crucial for aligning models with human perception, as humans can interpret abstract visual renditions, while machines tend to rely heavily on textural cues (hendrycks2020many; geirhos2018imagenet).

CLIP’s strong performance in several domains, including renditions, is attributed to its vast training distribution, rather than its contrastive learning objective, language supervision, or dataset size (fang2022data). However, fang2022data do not specify what characteristics of the training distribution drive this performance. CLIP could be learning more robust representations due to the diversity of natural images in its training set—or it may simply have been exposed to many datapoints from the (assumed to be OOD) test domains during training. Indeed, mayilvahanan2024 revealed that CLIP’s training data contains exact or near duplicates of samples of many OOD datasets. Yet, they showed that CLIP still generalizes well when this sample contamination is corrected. However, their analysis failed to account for domain contamination.

In contrast to sample contamination, domain contamination does not focus on duplicates of specific datapoints but rather examines whether critical aspects of a test domain are present in the training domain, such as images with different content but similar style to test samples. For example, after the correction by mayilvahanan2024, many other rendition images, while not duplicates, remained in CLIP’s training set (refer to Tab. LABEL:tab:domain_composition). Prior works often assume that CLIP is capable of generalizing OOD (radford2021learning; abbasi2024decipheringrolerepresentationdisentanglement; nguyen2024saftoutofdistributiongeneralizationfinetuning; fang2022data; li2023distillinglargevisionlanguagemodel; shu2023clipoodgeneralizingclipoutofdistributions); however, it remains unclear whether this is truly the case or if its performance is primarily driven by training on images from the test domain. This leads us to our central question:

To what extent does domain contamination explain CLIP’s performance on renditions?

We address the central question with the following contributions:

• Constructing Clean Single-Domain Datasets: To rigorously test whether CLIP’s success in the rendition domain stems from their exposure during training, we first train a domain classifier to distinguish natural images from renditions (Sec. 3.2). By applying the domain classifier to a deduplicated version of LAION-400M, we create and release two datasets: LAION-Natural contains 57M natural images; LAION-Rendition consists of 16M renditions of scenes and objects. Additionally, we refine existing rendition OOD benchmarks (ImageNet-R, ImageNet-Sketch, etc.) by removing samples that do not belong to the corresponding domain (LABEL:sec:subsampling_datasets).

• Refining the Evaluation of CLIP’s OOD Performance: Using LAION-Natural, we demonstrate that CLIP trained only on natural images significantly underperforms on rendition domain shifts (LABEL:sec:laion_nat_v_random). This suggests that its original success stems from domain contamination, not from an intrinsic OOD generalization ability (see Fig. 1 for a summary).

• Investigating Domain Mixing and Scaling Effects: Our single-domain datasets enable analyzing the effects of training on controlled mixtures of natural and rendition images across scales (LABEL:sec:laion_mix). We identify the optimal mixing ratio for the best overall performance and show the degree to which training on one domain enables some generalization to the other.

Through this work, we aim to shed light on the limitations of foundation models like CLIP in handling OOD generalization and provide valuable datasets and tools to the community for further exploration. Fig. 1 illustrates our core methodology.

Now, generate the 30 Q&A pairs in the specified JSON format based on the text provided above.