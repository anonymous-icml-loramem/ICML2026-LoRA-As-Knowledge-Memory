You are an expert academic assistant tasked with creating a high-quality question-answering dataset from a research paper's introduction. Your goal is to generate 30 question-and-answer pairs based exclusively on the provided text.

Instructions and Rules:
Source Grounding: All questions and answers MUST be derived solely from the provided introduction text. Do not use any external knowledge or make assumptions beyond what is written.

Question Hierarchy: You must create questions across three distinct levels of understanding, as defined below.

Quantity: Generate exactly 30 pairs in total: 10 for Level 1, 10 for Level 2, and 10 for Level 3.

Output Format: The output must be a single, valid JSON array of objects. Do not include any explanatory text, comments, or markdown formatting before or after the JSON code block.

Question Level Definitions:
Level 1: Key Information Recall (10 Questions)

Objective: To test the recall of specific, explicitly stated facts, proper nouns, terminology, and figures from the text.

Question Type: "What is...?", "What are the names of...?", "Which X was mentioned...?"

Level 2: Contextual Comprehension (10 Questions)

Objective: To test the understanding of relationships between concepts, such as cause-and-effect, problem-solution, comparisons, and the function of a component.

Question Type: "Why does...?", "What is the effect of A on B?", "How does X work?", "What is the difference between A and B?"

Level 3: Logical Structure Inference (10 Questions)

Objective: To test the understanding of the overall logical flow of the text, including identifying the core problem, the research gap, the proposed solution, and the main contribution.

Question Type: "What is the core problem the authors aim to solve?", "What research gap does this paper intend to fill?", "What is the main advantage of the proposed method?", "Summarize the key contribution of this work in relation to prior limitations."

Desired JSON Output Format:
The final output MUST be a JSON array containing 30 objects. Each object must have three keys: level (integer: 1, 2, or 3), question (string), and answer (string).

Example:

[
  {
    "level": 1,
    "question": "What is the full name of the algorithm the authors integrate their estimator into?",
    "answer": "The authors integrated their estimator into the Generalized Successive Elimination algorithm."
  },
  {
    "level": 2,
    "question": "According to the text, what is the inverse relationship between response time and preference strength?",
    "answer": "The text states that users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicate weaker preferences."
  },
  {
    "level": 3,
    "question": "What is the core reason complex psychological models are impractical for real-time systems, and how does this paper's proposal address it?",
    "answer": "They are impractical because they rely on computationally intensive methods like hierarchical Bayesian inference and MLE. This paper addresses it by proposing a computationally efficient method that frames utility estimation as a linear regression problem."
  }
]

Introduction Text to Analyze:
Title: An Analysis for Reasoning Bias of Language Models with Small Initialization

With the rapid advancement of deep learning technologies, Large Language Models have achieved remarkable success in the field of Natural Language Processing (NLP). These models have demonstrated exceptional capabilities across a wide range of tasks, from text generation to complex reasoning (Wei et al., 2022a; Achiam et al., 2023; Liu et al., 2024). Reasoning, in particular, is a critical ability for LLMs. A number of studies have focused on improving the reasoning ability of these models through data-driven approaches, such as RHO-1 (Lin et al., 2024) and Phi-4 (Abdin et al., 2024). However, there remains an ongoing debate as to whether LLMs genuinely learn the underlying logical rules or merely mimic patterns observed in the data (Marcus, 2003; Smolensky et al., 2022).

An alternative approach to enhancing the reasoning ability of LLMs focuses on the model architecture and its training process. In one such study examining the use of Transformers to model compositional functions, it was observed that the scale of model parameter initialization significantly impacts the modelâ€™s reasoning behavior (Zhang et al., 2024a, 2025). Specifically, smaller initialization scales bias the model toward fitting the data by learning primitive-level functions and compositional rules, whereas larger initialization scales tend to encourage memorization of input-output mappings. A qualitative rationale for this phenomenon has been proposed: with a small initialization, a well-documented effect known as neuron condensation emerges during training (Xu et al., 2025; Luo et al., 2021; Zhou et al., 2022; Zhang et al., 2022; Zhang & Xu, 2023; Zhang et al., 2023; Zhang & Xu, 2024). This phenomenon suggests that neurons within the same layer tend to behave similarly, promoting data fitting with the least possible complexity. To achieve a low-complexity result, the model must learn a minimal set of rules leading to capture the intrinsic primitive functions and compositional rules. However, this rationale does not reveal a critical question: how the optimization process, together with the Transformer structure, can achieve reasoning solutions with small initialization?

In this work, we identify a reasoning bias during the training of neural networks that learn natural language when initialized with small parameter scales. To illustrate this phenomenon, we employ a GPT-2 model (Radford et al., 2019) to train on a mixed dataset comprising two types of language data with distinct levels of reasoning complexity, within a single next-token prediction training framework. The first dataset, PrOntoQA (Saparov & He, 2023), consists of question-answering examples that include chains of thought, which explicitly describe the reasoning necessary to answer the questions correctly. The second dataset, TinyStories (Eldan & Li, 2023), is a synthetic corpus of short stories containing only words typically understood by children aged 3 to 4 years. As shown in Figure 1, the training loss for PrOntoQA decreases significantly faster than for TinyStories, suggesting that the model encounters and learns the reasoning patterns more readily.

We uncover a key mechanism whereby reasoning tasks are learned earlier during training because the tokens associated with these tasks become more differentiated in the embedding space at an early stage of the training process. We validate this mechanism using both synthetic data and real-world datasets. Furthermore, we provide a theoretical explanation for the evolution of token embeddings, which depends on the distribution of sample labels. Since each token is encoded as a one-hot vector, its embedding is adjusted based on the loss associated with the labels of that token. Consequently, different label distributions can lead to distinct learning behaviors for token embeddings. For memory tasks, the labels associated with each token are typically random and lack explicit structure, which results in similar distributions for different memory token labels. As a result, the embeddings for memory tokens are difficult to differentiate in the early stages of training. In contrast, reasoning tokens often exhibit distinct label distributions, leading to more differentiated embedding vectors for these tokens. These insights are elaborated through a simplified model using a multi-layer perceptron (MLP) and embedding structure, followed by an analysis of a Transformer model.

The primary contribution of this research lies in uncovering the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. By combining theoretical analysis with empirical evidence, we enhance the understanding of LLM training dynamics and provide new insights for optimizing model initialization strategies.

Now, generate the 30 Q&A pairs in the specified JSON format based on the text provided above.