You are an expert academic assistant tasked with creating a high-quality question-answering dataset from a research paper's introduction. Your goal is to generate 30 question-and-answer pairs based exclusively on the provided text.

Instructions and Rules:
Source Grounding: All questions and answers MUST be derived solely from the provided introduction text. Do not use any external knowledge or make assumptions beyond what is written.

Question Hierarchy: You must create questions across three distinct levels of understanding, as defined below.

Quantity: Generate exactly 30 pairs in total: 10 for Level 1, 10 for Level 2, and 10 for Level 3.

Output Format: The output must be a single, valid JSON array of objects. Do not include any explanatory text, comments, or markdown formatting before or after the JSON code block.

Question Level Definitions:
Level 1: Key Information Recall (10 Questions)

Objective: To test the recall of specific, explicitly stated facts, proper nouns, terminology, and figures from the text.

Question Type: "What is...?", "What are the names of...?", "Which X was mentioned...?"

Level 2: Contextual Comprehension (10 Questions)

Objective: To test the understanding of relationships between concepts, such as cause-and-effect, problem-solution, comparisons, and the function of a component.

Question Type: "Why does...?", "What is the effect of A on B?", "How does X work?", "What is the difference between A and B?"

Level 3: Logical Structure Inference (10 Questions)

Objective: To test the understanding of the overall logical flow of the text, including identifying the core problem, the research gap, the proposed solution, and the main contribution.

Question Type: "What is the core problem the authors aim to solve?", "What research gap does this paper intend to fill?", "What is the main advantage of the proposed method?", "Summarize the key contribution of this work in relation to prior limitations."

Desired JSON Output Format:
The final output MUST be a JSON array containing 30 objects. Each object must have three keys: level (integer: 1, 2, or 3), question (string), and answer (string).

Example:

[
  {
    "level": 1,
    "question": "What is the full name of the algorithm the authors integrate their estimator into?",
    "answer": "The authors integrated their estimator into the Generalized Successive Elimination algorithm."
  },
  {
    "level": 2,
    "question": "According to the text, what is the inverse relationship between response time and preference strength?",
    "answer": "The text states that users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicate weaker preferences."
  },
  {
    "level": 3,
    "question": "What is the core reason complex psychological models are impractical for real-time systems, and how does this paper's proposal address it?",
    "answer": "They are impractical because they rely on computationally intensive methods like hierarchical Bayesian inference and MLE. This paper addresses it by proposing a computationally efficient method that frames utility estimation as a linear regression problem."
  }
]

Introduction Text to Analyze:
Title: Inductive Moment Matching

Generative models for continuous domains have enabled numerous applications in images (Rombach et al., 2022; Saharia et al., 2022; Esser et al., 2024), videos (Ho et al., 2022a; Blattmann et al., 2023; OpenAI, 2024), and audio (Chen et al., 2020; Kong et al., 2020; Liu et al., 2023), yet achieving high-fidelity outputs, efficient inference, and stable training remains a core challenge — a trilemma that continues to motivate research in this domain. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b), one of the leading techniques, require many inference steps for high-quality results, while step-reduction methods, such as diffusion distillation (Yin et al., 2024; Sauer et al., 2025; Zhou et al., 2024; Luo et al., 2024a) and Consistency Models (Song et al., 2023; Geng et al., 2024; Lu & Song, 2024; Kim et al., 2023), often risk training collapse without careful tuning and regularization (such as pre-generating data-noise pair and early stopping).

To address the aforementioned trilemma, we introduce Inductive Moment Matching (IMM), a stable, single-stage training procedure that learns generative models from scratch for single- or multi-step inference. IMM operates on the time-dependent marginal distributions of stochastic interpolants (Albergo et al., 2023) — continuous-time stochastic processes that connect two arbitrary probability density functions (data at t=0 and prior at t=1). By learning a (stochastic or deterministic) mapping from any marginal at time t=1 to any marginal at time s<t, it can naturally support one- or multi-step generation (Figure 2).

IMM models can be trained efficiently from mathematical induction. For time s<r<t, we form two distributions at s by running a one-step IMM from samples at r and t. We then minimize their divergence, enforcing that the distributions at s are close to the distributions at r. This construction by induction guarantees convergence to the data distribution. To help with training stability, we model IMM based on certain stochastic interpolants and optimize the objective with stable sample-based divergence estimators such as moment matching (Gretton et al., 2012). Notably, we prove that Consistency Models (CMs) are a single-particle, first-moment matching special case of IMM, which partially explains the training instability of CMs.

On ImageNet-256x256, IMM surpasses diffusion models and achieves 1.99 FID with only 8 inference steps using standard transformer architectures. On CIFAR-10, IMM similarly achieves state-of-the-art of 1.98 FID with 2-step generation for a model trained from scratch.

Now, generate the 30 Q&A pairs in the specified JSON format based on the text provided above.