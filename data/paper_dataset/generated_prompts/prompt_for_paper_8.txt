You are an expert academic assistant tasked with creating a high-quality question-answering dataset from a research paper's introduction. Your goal is to generate 30 question-and-answer pairs based exclusively on the provided text.

Instructions and Rules:
Source Grounding: All questions and answers MUST be derived solely from the provided introduction text. Do not use any external knowledge or make assumptions beyond what is written.

Question Hierarchy: You must create questions across three distinct levels of understanding, as defined below.

Quantity: Generate exactly 30 pairs in total: 10 for Level 1, 10 for Level 2, and 10 for Level 3.

Output Format: The output must be a single, valid JSON array of objects. Do not include any explanatory text, comments, or markdown formatting before or after the JSON code block.

Question Level Definitions:
Level 1: Key Information Recall (10 Questions)

Objective: To test the recall of specific, explicitly stated facts, proper nouns, terminology, and figures from the text.

Question Type: "What is...?", "What are the names of...?", "Which X was mentioned...?"

Level 2: Contextual Comprehension (10 Questions)

Objective: To test the understanding of relationships between concepts, such as cause-and-effect, problem-solution, comparisons, and the function of a component.

Question Type: "Why does...?", "What is the effect of A on B?", "How does X work?", "What is the difference between A and B?"

Level 3: Logical Structure Inference (10 Questions)

Objective: To test the understanding of the overall logical flow of the text, including identifying the core problem, the research gap, the proposed solution, and the main contribution.

Question Type: "What is the core problem the authors aim to solve?", "What research gap does this paper intend to fill?", "What is the main advantage of the proposed method?", "Summarize the key contribution of this work in relation to prior limitations."

Desired JSON Output Format:
The final output MUST be a JSON array containing 30 objects. Each object must have three keys: level (integer: 1, 2, or 3), question (string), and answer (string).

Example:

[
  {
    "level": 1,
    "question": "What is the full name of the algorithm the authors integrate their estimator into?",
    "answer": "The authors integrated their estimator into the Generalized Successive Elimination algorithm."
  },
  {
    "level": 2,
    "question": "According to the text, what is the inverse relationship between response time and preference strength?",
    "answer": "The text states that users who strongly prefer to skip a product tend to do so quickly, while longer response times can indicate weaker preferences."
  },
  {
    "level": 3,
    "question": "What is the core reason complex psychological models are impractical for real-time systems, and how does this paper's proposal address it?",
    "answer": "They are impractical because they rely on computationally intensive methods like hierarchical Bayesian inference and MLE. This paper addresses it by proposing a computationally efficient method that frames utility estimation as a linear regression problem."
  }
]

Introduction Text to Analyze:
Title: Learning Action and Reasoning-Centric Image Editing from Videos and Simulations

Image editing is a complex task, involving many different skills, from adding/removing objects, changing colors/textures/styles, to “taking actions”: moving objects, changing actor positions or even more complex interactions. Tackling all of these requires fine-grained understanding of how visual scenes are composed as well as reasoning (e.g. spatial instructions or referring expressions). No current model can successfully do all of these edits, and most only perform localized changes involving object addition/removal or attribute edits, following the “inpainting paradigm” [Zhang et al., 2024, Xie et al., 2023]. Others have tried to address this issue by introducing more specialized model architectures which handle different editing subtasks [Couairon et al., 2023, Zhang et al., 2023a]. However, neither of these approaches includes edits requiring more holistic visual understanding of how humans and objects interact or how events unfold, such as ‘make the cook cut the apple in half’ or ‘make the dog jump in the air’ (see Fig. 1). These more action-centric edits are severely understudied in the space of instruction-tuned image editing models [Brooks et al., 2023, Huang et al., 2024]; when they are considered, it is done in isolation, ignoring other image edit subtasks and rigorous semantic evaluation [Soucek et al., 2023, Black et al., 2024]. In Sec. 2 we ˇdescribe a typology of these edit types and how existing datasets currently fail to address them all.
        
        As we argue in this paper, a major reason for these limitations is the lack of high-quality data. Finetuning data of object or attribute changes is simpler to acquire than other forms of edits, since inpainting setups directly leverage strong object and attribute abilities of txt2img models [Rombach et al., 2022] for paired-image data generation [Yildirim et al., 2023, Zhang et al., 2024]. However, solving the data scarcity for learning action and reasoning-centric edits is not as straightforward. We identify videos and simulation engines as the two most promising sources of data for these edit types. As we discuss in this paper, we find that previous models trained on “noisy” synthetic image pairs or video frames lead to poor editing abilities. Here, noisy refers to image pairs with changes not mentioned in the prompt, i.e. due to shortcomings of the automatic generation process or inherent properties of videos such as viewpoint changes and non-meaningful movement. Therefore, our main requirement of high-quality action and reasoning-centric edit examples is that they be truly minimal: Edited images which contain one or maximally two semantic changes described by the prompt, while all other aspects are kept exactly the same. From a diverse set of video sources and simulation engines, we curate the AURORA Dataset (Action-Reasoning-Object-Attribute). Via crowd-sourcing and curation we collect 130K truly-minimal examples from videos and 150K from simulation engines for instruction-tuned image editing. We describe our dataset and collection process in Sec. 3.
        
        The few image-text-alignment metrics commonly used in image editing are based on visual similarity to a groundtruth and in reality turn out to mostly measure the ability to stay maximally faithful (i.e. copying) to the source image [Zhang et al., 2024, Fu et al., 2023]. Though faithfulness is an important first step to master, these metrics have almost no correlation with the model’s ability to generate accurate edits, especially on action and reasoning-centric changes. Hence, in addition to the training data in AURORA, we introduce AURORA-BENCH(Sec. 4), a manually annotated benchmark covering 8 editing tasks on which we collect human judgement (Tab. 2). Inspired by work on image generation models as discriminators [Krojer et al., 2023, Li et al., 2023], we also describe a novel discriminative metric that assesses understanding and hallucination (Sec. 5.1). To demonstrate the efficacy and quality of AURORA, we present a state-of-the-art instruction-tuned image editing model, finetuned on AURORA and evaluated on AURORA-BENCH, which we compare to strong baselines in a set of experiments in Sec. 5.3.
        
        In summary our contributions are: 1) The creation of AURORA, a new clean and varied set of image edit pairs for instruction-finetuning that encompasses more action-centric and reasoningcentric examples. 2) We present a comprehensive benchmark covering a variety of edit types; 3) We introduce a novel more informative metric beyond existing ones; 4) We provide a state-of-the-art image editing model based on AURORA with well-rounded image editing capabilities covering object-centric, action-centric, and reasoning-centric edit abilities.

Now, generate the 30 Q&A pairs in the specified JSON format based on the text provided above.