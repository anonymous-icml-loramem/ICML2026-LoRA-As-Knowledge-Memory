text
"**Human Judgment in Enhancing Algorithmic Predictions**  
1. Introduction  
Despite the impressive progress in machine learning, human expertise remains essential in numerous high-stakes prediction scenarios. A compelling example is triage in emergency departments, where medical professionals evaluate and prioritize patients for urgent care. On one hand, predictive algorithms show great potential in enhancing triage decisions—indeed, algorithmic forecasts often surpass the accuracy of expert human judgment [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, algorithms may overlook the nuanced context of individual cases. For instance, an algorithmic risk score may rely only on structured data such as electronic health records or medical images, whereas a physician can gather additional insights through direct patient interaction.  

These two perspectives—algorithms frequently outperforming humans, yet humans having access to richer contextual information—are not mutually exclusive. Research in [9] confirms this dynamic in emergency room triage. This implies that even when algorithms excel, human input can still enhance their performance. Ideally, this collaboration could lead to human-AI complementarity [10, 11], where a combined system surpasses both individual components. Our study begins with the following question:  
*Under what conditions and how can human judgment enhance the predictions of any learning algorithm?*  

**Example: X-ray Classification**  
Consider the challenge of diagnosing atelectasis, a condition where part or all of the lung collapses (we explore this in detail in Section 5). Current state-of-the-art deep learning models can achieve strong performance using only chest X-rays as input [12, 13, 14]. We investigate whether incorporating a physician’s “second opinion” can further improve these predictions, especially since physicians may have access to unrecorded patient information through direct observation.  

One initial approach, without assuming any specific model type, is to ask whether a physician can differentiate between patients with identical imaging data. For instance, if a physician can accurately identify one patient with atelectasis while correctly ruling out another with the same X-ray, they must be accessing information not captured by the image. This could form the basis of a statistical test: we could determine if the physician performs better than chance in such scenarios. If so, even an algorithm that outperforms the physician might benefit from human input.  

However, finding identical observations in high-dimensional or continuous data—like X-rays—is rare. A practical alternative is to consider sufficiently ""similar"" observations, as proposed in [9]. In this paper, we introduce a more general concept of algorithmic indistinguishability, or coarser input subsets where no algorithm (from a user-defined class) has strong predictive power. We show that these subsets can be identified through a novel connection to multicalibration [15], and formally prove that human feedback within these subsets can yield better predictions than any algorithmic model in the same class.  

This framework is not only computationally feasible but also conceptually relevant from a decision-theoretic standpoint. While we have focused on the informational limitations of algorithms, it is also important to consider whether an expert's insights are simply difficult for the algorithm to learn due to constraints like limited data or computational power. Our approach bridges these contexts by defining indistinguishability based on the model class relevant to a specific prediction task. We now elaborate on our contributions.  

**Contributions**  
We introduce a new framework for human-AI collaboration in prediction tasks. Our method leverages human feedback to refine predictions within sets of inputs that are algorithmically indistinguishable—inputs that appear identical to predictive models. In Section 4, we present a straightforward method for incorporating feedback only when it improves upon the best available model and precisely measure the improvement. This extends the ""omnipredictors"" result of [16] in the case of squared error, which may be of independent interest.  

In Section 5, we present experiments showing that while humans generally do not outperform algorithmic predictors, there are specific instances—algorithmically indistinguishable cases—where human judgment is more accurate than the best available model, and these instances are identifiable beforehand.  

In Section 6, we examine a complementary scenario in which an algorithm provides recommendations to multiple downstream users who independently decide whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns, making it simultaneously optimal for all users."
"**Human Expertise in Enhancing Algorithmic Prediction**

**1. Introduction**

Although machine learning has made impressive strides, human judgment remains essential in many critical prediction scenarios. Take, for instance, the triage process in emergency rooms, where medical professionals determine the urgency of patient care. On one side, predictive algorithms show great potential in enhancing triage decisions—often outperforming even experienced human experts [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, algorithms may overlook the nuanced context that human clinicians can perceive. For example, while an algorithm might rely solely on structured data like electronic health records or medical images, a physician can also perform a direct physical exam, accessing a wealth of additional information.

These two perspectives—algorithms often performing better than humans, yet humans having access to a broader range of information—are not mutually exclusive. In fact, [9] found this very phenomenon in their analysis of emergency room triage. This implies that, even when algorithms excel, human input could still offer valuable improvements. Ideally, this collaboration could lead to human-AI complementarity [10, 11], where a combined system surpasses either individual component. This leads to our central question:

**When and how can human judgment enhance the predictions of any learning algorithm?**

**Example: X-ray Classification**

Consider the diagnosis of atelectasis, a condition where part or all of the lung collapses (we examine this task in detail in Section 5). Modern deep learning models can already achieve strong performance in classifying such conditions using only chest X-ray images [12, 13, 14]. We are interested in whether incorporating a second opinion from a physician can further improve these predictions, especially since physicians may observe information not captured in the X-ray itself.

One initial approach is to assess whether a physician can differentiate between patients with identical imaging data. For example, if a physician can correctly identify which of two patients with the same X-ray has atelectasis, they must possess information not present in the image. This could serve as a basis for a statistical test: we could evaluate whether the physician's performance is better than chance in such scenarios. If so, even an algorithm that outperforms the physician might still benefit from human input.

However, in high-dimensional or continuous data, such as X-rays, identical observations are rare. A more practical approach is to consider data that is sufficiently similar, as suggested by [9]. In this work, we introduce a more general concept of algorithmic indistinguishability—defining subsets of inputs where no algorithm (within a user-defined class) has significant predictive power. We show that these subsets can be identified using a novel connection to multicalibration [15], and formally demonstrate that incorporating human feedback for predictions within these subsets can outperform any algorithmic predictor in the same class.

Moreover, this framework is not only computationally feasible but also relevant from a decision-theoretic standpoint. While we've primarily focused on algorithmic constraints, it's also important to consider whether human expertise provides signals that are hard for algorithms to learn due to limitations in data or computational resources. Our approach naturally bridges these scenarios by defining indistinguishability relative to the model class most relevant to a specific prediction task. We elaborate on these contributions below.

**Contributions**

We introduce a novel framework for human-AI collaboration in prediction tasks. Our method uses human feedback to refine predictions within algorithmically indistinguishable input sets—inputs that appear identical to predictive algorithms. In Section 4, we present a simple method that incorporates this feedback only when it improves upon the best available model and precisely quantifies the improvement. This extends the ""omnipredictors"" result from [16] in the case of squared error, which may have independent significance. In Section 5, we present experiments showing that while humans generally do not outperform algorithms, there are specific, algorithmically indistinguishable cases where human judgment is more accurate than the best available model, and these cases can be identified in advance. Finally, in Section 6, we examine a complementary scenario where an algorithm recommends actions to multiple downstream users who independently decide whether to follow them. We provide conditions under which a predictor remains robust to these compliance patterns, thereby being optimal for all downstream users."
"**Human Judgment in Enhancing Algorithmic Predictions**  
1. **Introduction**  
Although machine learning has made impressive strides, human judgment remains essential in many high-stakes prediction scenarios. Take, for instance, the triage process in emergency rooms, where healthcare professionals evaluate and prioritize patients based on their condition. On one hand, predictive algorithms show great potential in improving triage decisions, often surpassing the accuracy of human experts [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, these algorithms may overlook crucial contextual information unique to each patient. For example, while an algorithm might rely solely on structured data like electronic health records or medical imaging, a physician can gather additional insights through direct patient examination.  

These two perspectives—algorithms often performing better than humans, yet humans having access to richer contextual data—are not mutually exclusive. In fact, [9] observed this very phenomenon in a study of emergency room triage. This suggests that even in cases where algorithms excel, human input could still enhance their performance. Ideally, this synergy could lead to human-AI complementarity [10, 11], where a combined system outperforms both individual components. Our work explores the following question:  
**When and how can human judgment enhance the predictive capabilities of any learning algorithm?**  

**Example: X-ray Classification**  
Consider the diagnosis of atelectasis, a condition where part or all of the lung collapses. Current state-of-the-art deep learning models can effectively classify this condition using only chest X-ray images [12, 13, 14]. We investigate whether incorporating a physician’s “second opinion” could further improve these predictions, especially since physicians may have access to additional patient information not captured in the X-ray.  

A basic approach is to ask whether a physician can differentiate between patients with identical imaging data. For example, if a physician can correctly identify one patient as having atelectasis and another as not, despite identical X-rays, they must possess information that the image does not contain. This could be tested statistically by assessing whether the physician’s performance is better than chance in distinguishing such cases. Even if an algorithm outperforms the physician, human input might still be beneficial.  

However, in real-world settings—particularly with continuous or high-dimensional data like X-rays—identical observations are rare. A practical alternative is to consider data that are “sufficiently similar,” as proposed in [9]. In this work, we introduce a broader concept of algorithmic indistinguishability: subsets of inputs where no algorithm (within a user-defined class) has strong predictive power. We demonstrate that these subsets can be identified through a novel connection to multicalibration [15], and show that using human feedback within these subsets can surpass the performance of any algorithmic predictor in the same class.  

This framework is not only computationally feasible but also has theoretical relevance from a decision-making perspective. While we have focused on the informational limitations of algorithms, it is also important to consider whether human experts provide signals that are difficult for algorithms to learn due to limited data or computational constraints. Our approach naturally bridges these scenarios by defining indistinguishability relative to the class of models most relevant to a specific task. We now elaborate on our key contributions.  

**Contributions**  
We introduce a new framework for human-AI collaboration in prediction tasks. Our method uses human feedback to refine predictions within sets of inputs that are indistinguishable to predictive algorithms. In Section 4, we present a simple technique for incorporating such feedback only when it improves upon the best available predictive model, and we quantify this improvement. This extends the ""omnipredictors"" result of [16] in the case of squared error, which may have broader applicability.  

In Section 5, we present experimental results showing that, although humans on average do not outperform algorithms, there are specific cases—algorithmically indistinguishable instances—where humans are more accurate than the best available predictor. These cases can be identified in advance.  

In Section 6, we examine the complementary scenario where an algorithm provides recommendations to multiple users, who independently decide whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns, thereby being optimal for all downstream users."
"**Leveraging Human Judgment in Algorithmic Prediction**

**1. Introduction**

Although machine learning has achieved impressive advancements, human judgment remains essential in many high-stakes prediction scenarios. Take, for instance, the process of triage in emergency rooms, where medical professionals assess and prioritize patients requiring immediate care. On one hand, predictive algorithms show great potential for enhancing triage decisions; in fact, algorithmic predictions can often surpass the accuracy of human experts [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, these algorithms may overlook important contextual information unique to each patient. For example, while an algorithm might rely on structured data like electronic health records or medical images, a physician can gather additional insights through direct patient interaction.

These two perspectives—algorithms often outperforming humans and humans accessing more comprehensive information—are not contradictory. In fact, [9] observed this very dynamic in an analysis of emergency room triage decisions. This implies that even when algorithms outperform humans, they could still benefit from human input. Ideally, this collaboration would lead to human-AI complementarity [10, 11], where a combined system outperforms either a human or an algorithm working independently. Our work is guided by the following question:

**When—and how—can human judgment enhance the predictions of any learning algorithm?**

**Example: X-ray Classification**

Consider the task of diagnosing atelectasis, a condition in which a lung is partially or fully collapsed (we explore this in detail in Section 5). Current state-of-the-art deep learning models can perform well on this classification task using only a patient’s chest X-ray as input [12, 13, 14]. We are interested in whether incorporating a ""second opinion"" from a physician can further improve these predictions, especially since a physician may have access to information—such as direct patient examination—that is not present in the X-ray.

A preliminary approach, without assuming any specific predictive model, is to determine whether a physician can differentiate between patients with identical imaging data. For instance, if a physician can accurately identify that one patient has atelectasis while another does not, despite having the same chest X-ray, it suggests that the physician has access to information not captured in the image. This could serve as a basis for a statistical test: we could evaluate whether the physician performs better than random when distinguishing between such patients. If so, even an algorithm that outperforms the physician may still benefit from human input.

However, in real-world settings, it is unlikely to find identical observations, especially in continuous or high-dimensional data like X-rays. A more practical approach is to consider observations that are sufficiently similar, as proposed by [9]. In this work, we introduce a broader concept of algorithmic indistinguishability—coarser subsets of inputs where no algorithm (within a user-defined class) can make meaningful predictions. We demonstrate that these subsets can be identified through a novel connection to multicalibration [15], and formally show that using human feedback to predict outcomes within these subsets can surpass any algorithmic predictor in the same class. 

This framework is not only computationally feasible but also relevant from a decision-theoretic standpoint. While we have focused on the informational limitations of algorithms, it is also important to consider whether an expert provides signals that are difficult for an algorithm to learn due to constraints like limited data or computational resources. Our approach naturally bridges these two perspectives by defining indistinguishability in relation to the model class that is most relevant for the specific prediction task. We elaborate on these contributions below.

**Contributions**

We introduce a novel framework for human-AI collaboration in prediction tasks. Our method utilizes human feedback to refine predictions within sets of inputs that are algorithmically indistinguishable, meaning they ""look the same"" to predictive algorithms. In Section 4, we present a simple method for incorporating this feedback only when it improves upon the best available model, and we precisely quantify the resulting improvement. This extends the ""omnipredictors"" result of [16] in the special case of squared error, which may have independent interest. In Section 5, we present experimental results showing that while humans may not outperform algorithms on average, there are specific instances (algorithmically indistinguishable) where human accuracy exceeds that of the best available predictor, and these instances can be identified in advance. In Section 6, we explore the complementary scenario in which an algorithm provides recommendations to multiple users, who independently decide whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns, thereby being simultaneously optimal for all downstream users."
"**Human Judgment in Enhancing Algorithmic Predictions**  
**1. Introduction**  
Even with significant progress in machine learning, human expertise remains vital in numerous high-stakes prediction scenarios. Take, for instance, the triage process in emergency rooms, where medical professionals evaluate and prioritize patients for urgent care. On one hand, predictive algorithms have shown great potential in enhancing triage decisions, often surpassing human experts in accuracy [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, these algorithms may not fully account for the nuanced context of individual cases. For example, an algorithm might rely solely on structured data like electronic health records or medical images, while a physician can gather additional insights through direct patient interaction.

These two perspectives—algorithms often outperforming humans, yet humans having access to richer contextual information—are not mutually exclusive. In fact, [9] observed this phenomenon in their analysis of emergency room triage. This implies that, even when algorithms excel, they can still benefit from human input. Ideally, this collaboration would lead to human-AI complementarity [10, 11], where a combined system outperforms either component working alone. Our research begins with the following question:  
**When and how can human judgment enhance the predictions of any learning algorithm?**

**Example: X-ray Classification**  
Consider the challenge of diagnosing atelectasis (a condition where a lung is partially or fully collapsed; we explore this in detail in Section 5). Current state-of-the-art deep learning models can achieve strong performance by analyzing chest X-rays alone [12, 13, 14]. We investigate whether incorporating a physician's “second opinion” could further improve these predictions, especially since a physician might have access to additional information, such as direct patient examination, that the X-ray does not capture.

A straightforward approach, without assuming any specific model, is to ask whether a physician can differentiate between patients with identical imaging data. If a physician can correctly identify one patient as having atelectasis while another does not, despite identical X-rays, it suggests they have access to information not included in the images. This could serve as the basis for a statistical test: we could assess whether the physician performs better than chance in distinguishing such cases. If so, even an algorithm that outperforms the physician might still benefit from human input.

However, in real-world settings, especially with high-dimensional or continuous data like X-rays, identical observations are rare. A practical alternative is to consider inputs that are sufficiently similar, as proposed by [9]. In this work, we introduce a broader concept of algorithmic indistinguishability—coarser groups of inputs where no algorithm (from a user-defined class) can make meaningful predictions. We demonstrate that these subsets can be identified through a novel connection to multicalibration [15], and formally show that using human feedback to predict outcomes within these subsets can outperform any algorithmic predictor from the same class.

This framework is not only computationally feasible but also relevant from a decision-theoretic standpoint. While we’ve focused on the informational limitations of algorithms, it’s also important to consider whether experts provide signals that are inherently difficult for algorithms to learn due to factors like limited data or computational constraints. Our approach bridges these scenarios by defining indistinguishability based on the most relevant model class for a given prediction task. We elaborate on these contributions below.

**Contributions**  
We introduce a new framework for human-AI collaboration in prediction tasks. Our method leverages human feedback to refine predictions within sets of inputs that are indistinguishable to algorithms, or ""look the same"" from the perspective of predictive models. In Section 4, we present a straightforward method to incorporate this feedback only when it improves upon the best available predictive model, and we precisely measure this improvement. This extends the “omnipredictors” result of [16] in the case of squared error, which may have independent value. In Section 5, we present experiments showing that although humans generally underperform algorithms on average, there are specific instances—algorithmically indistinguishable cases—where humans are more accurate than the best available model, and these instances can be identified in advance. In Section 6, we explore a complementary scenario in which an algorithm recommends actions to multiple users who decide independently whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns, making it simultaneously optimal for all downstream users."
"**Human Judgment in Enhancing Algorithmic Predictions**

**1. Introduction**

Even with significant advancements in machine learning, human judgment remains essential in many high-stakes prediction scenarios. Consider, for instance, the process of triage in an emergency room, where medical professionals evaluate and prioritize patients requiring urgent care. On one hand, predictive algorithms show great potential in enhancing triage decisions, often proving more accurate than human experts alone [1–8]. On the other hand, these algorithms may miss the nuanced contextual details that a human can provide. For example, while an algorithm may rely solely on structured data like electronic health records or medical images, a physician can incorporate additional insights—such as direct patient examination—which are not captured in the data.

These two perspectives—algorithms often surpassing human performance, yet humans having access to richer contextual information—are not mutually exclusive. A study [9] confirms this dual dynamic in emergency room triage. This implies that even when algorithms excel, they could still benefit from human input. Ideally, this collaboration could lead to **human-AI complementarity** [10, 11], where a combined system surpasses either human or algorithmic performance individually. Our research begins by asking:

**When and how can human judgment enhance the predictions of any learning algorithm?**

**Example: X-ray Classification**

Take the task of diagnosing atelectasis (a condition where a lung partially or fully collapses), which we examine in detail in Section 5. Current state-of-the-art deep learning models can effectively classify this condition using only chest X-ray images [12–14]. However, we are interested in whether adding a “second opinion” from a physician could further improve these predictions. This is especially relevant since physicians may have access to additional patient information—such as direct observation—which is not present in the X-ray image.

One initial approach, without assuming specific predictive models, is to ask whether a physician can differentiate between patients with identical imaging data. For instance, if a physician can accurately identify one patient with atelectasis and another without, despite identical X-rays, they must be using information not captured in the image. In theory, this could serve as a basis for a statistical test: we could evaluate whether the physician performs better than chance in distinguishing such cases. If so, even an algorithm that outperforms the physician may still benefit from human input.

However, it's unlikely to find identical data in continuous or high-dimensional formats like X-rays. A natural alternative is to consider ""sufficiently similar"" observations, as suggested in [9]. In this work, we propose a more general concept of **algorithmic indistinguishability**, or subsets of inputs where no algorithm (from a user-defined class) has strong predictive power. We demonstrate that these subsets can be identified through a novel connection to **multicalibration** [15], and formally show that using human feedback to predict outcomes within these subsets can outperform any algorithmic predictor from the same class.

This framework is not only computationally feasible but also has theoretical relevance from a decision-making standpoint. While we’ve focused on the informational limitations of algorithms, it's also important to consider whether human experts provide signals that are simply hard for algorithms to learn due to limited data or computational constraints. Our approach naturally bridges these two contexts by defining indistinguishability relative to the model class most relevant to a given task.

**Contributions**

We introduce a new framework for enhancing prediction tasks through human-AI collaboration. Our method leverages human feedback to refine predictions within sets of inputs that are indistinguishable to algorithms—inputs that “look the same” from a predictive perspective. In Section 4, we present a straightforward method for incorporating this feedback only when it improves upon the best available model, and we quantify the improvement. This extends the ""omnipredictors"" result from [16] in the context of squared error, which may have independent applications.

In Section 5, we present experimental results showing that while humans on average do not outperform algorithmic predictors, there are specific cases—algorithmically indistinguishable instances—where human accuracy exceeds that of the best available model, and these cases can be identified in advance.

Finally, in Section 6, we explore the complementary scenario where an algorithm provides recommendations to multiple users, who independently decide whether to follow them. We establish conditions under which a predictor is robust to these compliance behaviors, ensuring it remains optimal for all downstream users."
"**Human Judgment in Algorithmic Prediction**

**1. Introduction**

Although machine learning has achieved impressive advancements, human judgment remains essential in many high-stakes prediction scenarios. Take, for instance, the challenge of triage in emergency departments, where medical professionals assess and prioritize patients based on urgency. On one side, predictive algorithms show great potential for enhancing triage decisions; in fact, they often surpass even experienced human clinicians in accuracy [1, 2, 3, 4, 5, 6, 7, 8]. However, these algorithms may not fully account for the nuanced context of each individual case. For example, an algorithm might rely solely on structured data such as electronic health records or medical images, whereas a physician can gather additional insights through direct patient interaction.

This dual observation—that algorithms often outperform humans but humans have access to richer contextual information—is not mutually exclusive. In fact, [9] found this pattern in their analysis of triage decisions in emergency rooms. This implies that, even in cases where algorithms excel, human input could still enhance their performance. Ideally, this synergy could lead to human-AI complementarity [10, 11], where a combined system surpasses either human or algorithmic performance individually. Our research begins with the following question:

**When (and how) can human judgment enhance the accuracy of predictive algorithms?**

**Example: X-ray classification.** Consider the task of diagnosing atelectasis (a condition where a lung is partially or fully collapsed; we explore this in detail in Section 5). Current deep learning models can achieve strong performance in classifying this condition using only chest X-rays as input [12, 13, 14]. We are interested in whether we can further improve these predictions by incorporating a ""second opinion"" from a physician, especially since physicians may have access to information not captured in the X-ray, such as direct patient observation.

A first approach, without assuming specific predictive models, is to ask whether a physician can differentiate between patients with identical imaging data. For example, if a physician correctly identifies one patient as having atelectasis while another does not—despite their chest X-rays being identical—then the physician must be using information not present in the image. This could form the basis of a statistical test: we could evaluate whether the physician performs better than random when distinguishing such patients. If so, even an algorithm that outperforms the physician might still benefit from human input.

Of course, in real-world settings, we rarely encounter exactly identical observations, especially in high-dimensional or continuous data like X-rays. A more practical approach is to consider observations that are sufficiently ""similar,"" as suggested by [9]. In this work, we introduce a broader concept of *algorithmic indistinguishability*, or subsets of inputs where no algorithm (within a user-defined class) can reliably predict outcomes. We show how these subsets can be identified through a novel connection to *multicalibration* [15], and formally demonstrate that using human feedback within these subsets can outperform any algorithmic predictor from the same class. Additionally, this framework is not only computationally feasible but also relevant from a decision-theoretic perspective. While we have focused on the informational limitations of algorithms, it is also natural to consider whether experts provide signals that are simply difficult for algorithms to learn due to limited data or computational constraints. Our approach naturally bridges these scenarios by defining indistinguishability based on the class of models most relevant for a given task. We elaborate on these contributions below.

**Contributions.** We introduce a new framework for human-AI collaboration in prediction tasks. Our method uses human feedback to refine predictions within sets of inputs that are algorithmically indistinguishable—inputs that appear identical to predictive models. In Section 4, we present a simple method to apply this feedback only when it improves upon the best feasible model, and we precisely quantify this improvement. This extends the ""omnipredictors"" result of [16] in the case of squared error, which may be of independent interest. In Section 5, we present experiments showing that while humans generally do not outperform algorithmic models on average, there are specific, algorithmically indistinguishable instances where human judgment is more accurate than the best available predictor—and these instances can be identified in advance. In Section 6, we explore a complementary scenario where an algorithm provides recommendations to multiple users, who independently decide whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns, making it simultaneously optimal for all downstream users."
"**Human Judgment in Enhancing Algorithmic Predictions**  
**1. Introduction**  
Although machine learning has made significant strides, human expertise remains essential in many high-stakes prediction scenarios. Take, for example, the triage process in emergency rooms, where medical professionals determine the priority of patient care. On one hand, predictive algorithms have shown great potential in enhancing decision-making in such contexts; in fact, they often surpass the accuracy of human experts [1, 2, 3, 4, 5, 6, 7, 8]. On the other hand, these algorithms may not fully account for the unique circumstances of each individual. For instance, while an algorithm may rely solely on structured data like electronic health records or imaging, a physician can gather additional insights through direct patient interaction.

These two perspectives—algorithms frequently outperforming humans, yet humans having access to richer contextual information—are not mutually exclusive. A study by [9] highlights this very phenomenon in emergency room triage. This suggests that even when algorithms excel, human input could still enhance their performance. Ideally, this collaboration could lead to human-AI complementarity [10, 11], where a combined system surpasses both individual components. Our research begins with a key question:

**When—and how—can human judgment enhance the predictions of any learning algorithm?**

**Example: X-ray Classification**  
Consider the task of diagnosing atelectasis, a condition where part or all of the lung collapses (we explore this in detail in Section 5). Current deep learning models can achieve strong performance by analyzing chest X-rays alone [12, 13, 14]. We investigate whether incorporating a physician's “second opinion” could further improve these predictions. This is especially relevant, as physicians may observe additional information, such as a patient’s physical condition, that is not captured in the X-ray.

A basic approach is to determine whether a physician can differentiate between patients with identical imaging data. For example, if a physician can accurately identify one patient as having atelectasis while another does not—despite identical X-rays—then the physician must have access to information the X-ray lacks. This could form the basis of a statistical test: we could assess whether the physician performs better than chance in such cases. If so, even an algorithm that outperforms the physician might still benefit from human input.

However, identical data is rare in continuous or high-dimensional inputs like X-rays. A practical alternative is to consider data that are sufficiently similar, as suggested by [9]. In this work, we introduce a broader concept of algorithmic indistinguishability—coarser groupings of inputs where no algorithm (from a user-defined class) can make meaningful predictions. We show that these groups can be identified using a novel connection to multicalibration [15], and we formally demonstrate that human feedback can lead to better predictions within these groups than any algorithmic model in the same class. 

This framework is not only computationally feasible but also relevant from a decision-theoretic standpoint. While we’ve focused on the information limitations of algorithms, it’s also important to consider whether human input provides signals that are hard for algorithms to learn due to constraints like limited data or computational resources. Our approach bridges these contexts by defining indistinguishability based on the model class that is most relevant for a specific task. We elaborate on our contributions below.

**Contributions**  
We introduce a novel framework for integrating human judgment into AI prediction systems. Our method leverages human feedback to refine predictions within groups of inputs that are algorithmically indistinguishable—inputs that appear identical to predictive models. In Section 4, we present a straightforward method that applies human feedback only when it improves upon the best available model, and we precisely quantify this improvement. This extends the ""omnipredictors"" result from [16] in the context of squared error, which may be of independent interest. In Section 5, we present experiments showing that while humans do not outperform algorithms on average, there are specific instances within algorithmically indistinguishable groups where human accuracy surpasses that of the best available model—instances that can be identified in advance. In Section 6, we examine a complementary scenario where an algorithm provides recommendations to multiple users who independently decide whether to follow them. We establish conditions under which a model remains robust to these compliance patterns, thus being optimal for all users simultaneously."
"**Human Judgment in Enhancing Algorithmic Predictions**  
1. **Introduction**  
Even with significant progress in machine learning, human insight remains essential in many high-stakes predictive scenarios. Consider, for instance, the challenge of triage in emergency rooms, where medical professionals determine the priority of patient care. While predictive algorithms show great potential in improving such decisions—often surpassing even the accuracy of experienced clinicians [1–8]—they may overlook critical contextual information that only human experts can provide. For example, a risk score generated by an algorithm might rely solely on structured data like electronic health records or medical images, whereas a physician can observe a patient directly and consider nuanced factors that are not captured in the data.  

These two perspectives—algorithms often outperforming humans and humans having access to richer information—are not mutually exclusive. In fact, [9] found this very dynamic in their analysis of emergency room triage. This suggests that even when algorithms excel, they can still benefit from human input. Ideally, this collaboration could lead to **human-AI complementarity** [10, 11], where a combined system performs better than either a human or an algorithm working alone. Our study begins by addressing this key question:  
**When and how can human judgment enhance the predictions of any learning algorithm?**  

**Example: X-ray Classification**  
Take the task of diagnosing atelectasis, a condition where part or all of the lung collapses. Modern deep learning models have achieved strong performance in classifying this condition based solely on chest X-rays [12–14]. However, we are interested in whether incorporating a physician’s “second opinion” could further improve these predictions, especially since physicians may have access to additional information not captured in the X-ray, such as direct patient observation.  

A simple starting point is to ask whether a physician can distinguish between patients whose X-rays are identical. If a physician can accurately identify one patient with atelectasis and another without, despite identical imaging, this indicates the presence of unrecorded information. This could be tested statistically: we could assess whether the physician performs better than chance in such cases. If so, even an algorithm that outperforms the physician might still gain from human input.  

Of course, identical data points are rare in real-world settings, particularly with high-dimensional or continuous data like X-rays. A practical alternative is to consider data that is “sufficiently similar,” as suggested by [9]. In this work, we introduce a broader concept of **algorithmic indistinguishability**, defining subsets of inputs where no algorithm (from a user-specified class) has strong predictive power. We show that these subsets can be identified using a novel link to **multicalibration** [15], and formally demonstrate that using human feedback to predict outcomes within these subsets can outperform any algorithmic predictor in the same class.  

Beyond being computationally feasible, this framework has important implications from a decision-theoretic standpoint. While we’ve focused on algorithmic limitations, it's also natural to ask whether expert input provides signals that are simply difficult for algorithms to learn due to limited data or computational constraints. Our approach bridges these scenarios by defining indistinguishability based on the model class relevant to each prediction task. We elaborate on these contributions below.  

**Contributions**  
We introduce a new framework for integrating human judgment into algorithmic prediction tasks. Our method uses human feedback to refine predictions within algorithmically indistinguishable input sets—those that appear the same to predictive models. In Section 4, we present a straightforward method for incorporating this feedback only when it improves upon the best feasible predictive model, and we precisely quantify this improvement. This extends the “omnipredictors” result from [16] in the case of squared error, which may be of independent interest.  

In Section 5, we present experimental results showing that although humans do not generally outperform algorithms on average, there are specific instances—algorithmically indistinguishable cases—where humans are more accurate than the best available predictor. These instances are identifiable beforehand.  

Finally, in Section 6, we examine a complementary scenario in which an algorithm provides recommendations to multiple downstream users, who independently decide whether to follow them. We establish conditions under which a predictor remains robust to these compliance patterns and is thus optimal for all users."
"**Human Judgment in Enhancing Algorithmic Predictions**

**1. Introduction**

Although machine learning has made impressive strides, human judgment remains essential in many high-stakes prediction scenarios. Take, for instance, the task of triage in an emergency room, where medical professionals evaluate and prioritize patients based on their condition. While algorithmic models show great potential for enhancing decision-making—often surpassing even expert human assessments [1, 2, 3, 4, 5, 6, 7, 8]—they may lack the contextual awareness that a physician provides. For example, an algorithm might rely solely on structured data like electronic health records or medical images, while a doctor can also observe the patient directly, gaining critical insights not captured in the data.

This dynamic—where algorithms often outperform humans but humans have access to a broader range of information—does not imply a contradiction. In fact, [9] found this pattern in their analysis of emergency room triage. This suggests that even when algorithms excel, human input can still offer valuable contributions. Ideally, this collaboration can lead to **human-AI complementarity** [10, 11], where a combined system outperforms either a human or an algorithm working alone. Our research begins with the following question:

**When (and how) can human judgment enhance the accuracy of any learning algorithm?**

**Example: X-ray Classification**

Consider the diagnosis of atelectasis, a condition where part or all of a lung collapses. Current state-of-the-art deep learning models can effectively classify this condition using only chest X-rays as input [12, 13, 14]. We investigate whether incorporating a physician’s “second opinion” can further improve these predictions, especially since physicians may have access to additional information—such as direct patient examination—that is not captured in the X-ray.

A simple starting point is to ask whether a physician can discern differences between patients with identical imaging data. If a doctor can correctly identify atelectasis in one patient while ruling it out in another, despite identical X-rays, it indicates the presence of information not captured by the images. This could be used as a basis for a statistical test: we could determine whether the physician's performance is better than chance in such cases. If so, even an algorithm that outperforms the physician might still benefit from human input.

However, in real-world settings with continuous or high-dimensional data, such as X-rays, exact matches are rare. A more practical approach is to consider inputs that are sufficiently similar, as suggested by [9]. In this paper, we introduce a more general concept of **algorithmic indistinguishability**, referring to subsets of inputs where no algorithm (from a user-defined class) can reliably predict outcomes. We demonstrate that these subsets can be identified through a novel connection to **multicalibration** [15], and show that incorporating human feedback within these subsets can lead to better prediction performance than any algorithm in the same class. 

This framework is not only computationally feasible but also relevant from a decision-theoretic standpoint. While we have focused on the informational limitations of algorithms, it is also possible that human input provides signals that are hard for algorithms to learn due to limited training data or computational constraints. Our approach bridges these scenarios by defining indistinguishability based on the class of models most relevant for a specific prediction task.

**Contributions**

We introduce a novel framework for human-AI collaboration in prediction tasks. Our method leverages human feedback to refine predictions within algorithmically indistinguishable sets—inputs that appear the same to predictive models. In Section 4, we present a straightforward method to apply this feedback only when it improves upon the best available model, and we quantify the extent of this improvement. This extends the ""omnipredictors"" result of [16] in the case of squared error, which could have broader applications.

In Section 5, we present experimental results showing that while humans do not, on average, outperform algorithmic predictors, there are specific instances—algorithmically indistinguishable cases—where they are more accurate than the best available model. Importantly, these instances can be identified in advance.

In Section 6, we explore a complementary scenario where an algorithm provides recommendations to multiple users, who independently decide whether to follow them. We establish conditions under which a predictor is robust to these compliance patterns, ensuring it remains optimal for all downstream users."
"Question: What is the main question that the authors of the text are trying to address? Answer: The main question the authors are trying to address is: ""When (and how) can human judgment improve the predictions of any learning algorithm?"""
"Question: According to the text, what is one example of a high-stakes prediction task where human judgment is still critical? Answer: One example of a high-stakes prediction task where human judgment is still critical is triage in the emergency room, where healthcare providers assess and prioritize patients for immediate care."
"Question: What is the key difference between algorithmic predictions and human judgment in the context of triage? Answer: The key difference is that algorithmic predictions may not fully capture the relevant context for each individual, while human judgment can incorporate additional modalities such as direct patient examination."
"Question: What does the text suggest about the relationship between algorithmic performance and human input in prediction tasks? Answer: The text suggests that even in settings where algorithms outperform humans, algorithms might still benefit from human input, potentially leading to human-AI complementarity."
"Question: What is the example used in the text to illustrate the potential benefit of human input in algorithmic predictions? Answer: The example used is X-ray classification for diagnosing atelectasis, where a physician may have access to information not present in the X-ray."
"Question: What is the first heuristic proposed in the text to determine if a physician can provide useful information beyond what an algorithm can capture? Answer: The first heuristic proposed is to ask whether a physician can distinguish patients whose imaging data are identical, indicating that the physician has information not captured by the X-ray."
"Question: What is the concept of ""algorithmic indistinguishability"" introduced in the text? Answer: Algorithmic indistinguishability refers to subsets of inputs in which no algorithm (in some rich, user-defined class) has significant predictive power."
Question: How does the text propose to discover these subsets of algorithmically indistinguishable inputs? Answer: The text proposes a novel connection to multicalibration to discover these subsets of algorithmically indistinguishable inputs.
Question: What is the benefit of using human feedback within these subsets of algorithmically indistinguishable inputs? Answer: Using human feedback within these subsets can outperform any algorithmic predictor in the same user-defined class.
"Question: What is the significance of the ""omnipredictors"" result mentioned in the text? Answer: The ""omnipredictors"" result is significant because it is extended in the special case of squared error, which may be of independent interest."
Question: What does the text say about the performance of humans compared to algorithmic predictors on average? Answer: The text states that humans fail to outperform algorithmic predictors on average.
"Question: What is the implication of the text regarding specific instances where humans may outperform algorithms? Answer: The text implies that there exist specific (algorithmically indistinguishable) instances on which humans are more accurate than the best available predictor, and these instances are identifiable ex ante."
"Question: What is the complementary setting discussed in the text where an algorithm provides recommendations to many users? Answer: The complementary setting discussed is where an algorithm provides recommendations to many downstream users, who independently choose when to comply."
"Question: What conditions does the text provide for a predictor to be robust to compliance patterns from users? Answer: The text provides conditions under which a predictor is robust to these compliance patterns, allowing it to be simultaneously optimal for all downstream users."
"Question: What is the main contribution of the authors' work according to the text? Answer: The main contribution is a novel framework for human-AI collaboration in prediction tasks, using human feedback to refine predictions within sets of algorithmically indistinguishable inputs."
Question: What is the significance of the framework proposed in the text from a decision-theoretic perspective? Answer: The framework is significant from a decision-theoretic perspective because it addresses both the informational constraints of algorithms and the potential for experts to provide difficult-to-learn signals.
"Question: What is the role of the user-defined class of models in the proposed framework? Answer: The user-defined class of models is used to define indistinguishability, allowing the framework to be adaptable to different prediction tasks."
"Question: What is the purpose of the experiments presented in Section 5 of the text? Answer: The purpose of the experiments in Section 5 is to demonstrate that although humans fail to outperform algorithmic predictors on average, there are specific instances where humans are more accurate."
"Question: What is the implication of the text regarding the integration of human and algorithmic predictions? Answer: The implication is that integrating human and algorithmic predictions can lead to better outcomes, especially in cases where human judgment provides additional context or information not captured by algorithms."
"Question: What is the overall goal of the proposed framework according to the text? Answer: The overall goal of the proposed framework is to enable human-AI collaboration in prediction tasks, where human feedback can improve algorithmic predictions within sets of algorithmically indistinguishable inputs."
"Summary: Human expertise continues to play a vital role in high-stakes prediction tasks, even with the advancement of machine learning. For instance, in emergency room triage, while algorithmic predictions often outperform human judgment, they may lack the contextual understanding that physicians provide through direct patient examination. These two perspectives are not mutually exclusive; studies show that human input can still enhance algorithmic predictions. This leads to the concept of human-AI complementarity, where collaboration between humans and algorithms can yield better outcomes than either working alone. The central question of this work is: *When (and how) can human judgment improve the predictions of any learning algorithm?*

To explore this, the text uses the example of X-ray classification for diagnosing atelectasis. While deep learning models perform well using chest X-rays alone, incorporating a physician’s “second opinion” could improve accuracy, especially if the physician has access to additional patient information. A heuristic approach involves testing whether physicians can distinguish between patients with identical imaging data, suggesting they may possess information not captured by the X-ray. The text proposes a generalized notion of algorithmic indistinguishability, where inputs are considered similar if no algorithm in a defined class can predict outcomes effectively. This concept is linked to multicalibration, and the framework shows that human feedback can outperform algorithmic predictions within these subsets. The study also considers scenarios where human judgment may be difficult for algorithms to learn due to data or computational limitations. The paper contributes a novel framework for human-AI collaboration, demonstrating how human feedback can refine predictions in algorithmically indistinguishable cases and ensuring robustness in settings where algorithmic recommendations influence multiple users."
"Summary: The text explores the role of human expertise in enhancing algorithmic prediction, particularly in high-stakes scenarios like emergency room triage. While algorithms often outperform humans in accuracy, they may miss contextual information that humans can provide. This highlights the potential for human-AI collaboration, aiming to achieve complementarity where combined efforts surpass individual performance. The question posed is: when and how can human judgment improve algorithmic predictions?

An example is the diagnosis of atelectasis using chest X-rays. Although deep learning models perform well, incorporating a physician's second opinion could enhance accuracy by utilizing additional patient information not captured in the images. A heuristic involves checking if a physician can distinguish between identical imaging data, suggesting the presence of unrecorded information. This leads to the concept of algorithmic indistinguishability, where certain input subsets are not predictive for any algorithm. The text proposes a framework leveraging multicalibration to identify such subsets, enabling human feedback to improve predictions within these groups.

The contributions include a method for integrating human feedback only when it enhances predictions, extending prior work on omnipredictors. Experiments show that while humans may not outperform algorithms on average, they can be more accurate on specific, indistinguishable cases. Additionally, the text considers scenarios where algorithms guide users, and provides conditions for robustness against compliance patterns. Overall, the work emphasizes the importance of human input in refining algorithmic predictions, especially in complex and context-rich environments."
"Summary: Human expertise continues to play a vital role in high-stakes prediction tasks, even with significant advancements in machine learning. In settings like emergency room triage, algorithms often outperform human experts in accuracy, yet human judgment can provide critical contextual insights not captured by structured data. This does not imply conflict; rather, it suggests the potential for human-AI collaboration to achieve better outcomes. The paper explores how human input can improve algorithmic predictions, particularly in tasks like diagnosing atelectasis from chest X-rays. While deep learning models perform well on such tasks, they may benefit from a physician's additional insights, especially when X-rays are identical. The study introduces a framework for identifying algorithmically indistinguishable inputs—sets of data where no algorithm has significant predictive power—and proposes a method to use human feedback within these sets to enhance predictions. This approach is grounded in multicalibration and is applicable across various model classes. The paper highlights contributions including a method for incorporating human feedback only when beneficial, experimental results showing human accuracy in specific cases, and conditions for robust algorithmic recommendations in the presence of user compliance variability. The work aims to bridge human and algorithmic strengths for improved prediction."
"Summary: Human expertise continues to play a crucial role in high-stakes prediction tasks, even with the remarkable progress in machine learning. For instance, in emergency room triage, while algorithmic predictions often outperform human experts in accuracy, they may miss important contextual information that physicians can gather through direct patient examination. This does not imply a conflict between human judgment and algorithmic prediction, as evidenced by studies showing that human input can enhance algorithmic performance. The key question is: *When and how can human judgment improve algorithmic predictions?* 

A notable example is the diagnosis of atelectasis using chest X-rays. Although deep learning models perform well on such tasks, they may benefit from a physician's ""second opinion,"" especially when the physician has access to additional patient information. A simple heuristic is to determine whether a physician can distinguish between patients with identical X-rays, which could indicate the presence of unrecorded information. However, in high-dimensional data like X-rays, identical observations are rare, so the concept of algorithmic indistinguishability is introduced. This involves identifying subsets of inputs where no algorithm has significant predictive power, and using human feedback within these subsets can outperform algorithmic predictors. The work presents a framework for human-AI collaboration, demonstrating that human feedback can improve predictions, even in cases where algorithms are otherwise superior. The contributions include a method to incorporate human feedback effectively, experimental results showing human accuracy on specific instances, and conditions for algorithmic robustness in recommendation settings."
"Summary: The text explores the role of human expertise in enhancing algorithmic predictions, especially in high-stakes settings like emergency room triage. It acknowledges that while machine learning algorithms often outperform humans in accuracy, human judgment can provide crucial contextual information that algorithms may miss. This is illustrated through the example of diagnosing atelectasis using chest X-rays, where physicians may have access to additional patient information not captured in the images. The text proposes a framework for human-AI collaboration, focusing on algorithmically indistinguishable inputs—cases where no algorithm can reliably predict outcomes. It introduces a method to incorporate human feedback in such scenarios, leveraging a concept called multicalibration. The framework is designed to be adaptable, allowing for the integration of human input based on the specific predictive models used. The study highlights that while humans may not consistently outperform algorithms on average, they can be more accurate in certain indistinguishable cases. Furthermore, it examines scenarios where algorithmic recommendations are followed by multiple users, and provides conditions for ensuring the robustness of predictions in such settings. The contributions include a novel approach to human-AI collaboration, empirical validation through experiments, and theoretical insights into the effectiveness of human input in prediction tasks."
"Summary: The text explores the role of human judgment in enhancing algorithmic predictions, particularly in high-stakes tasks like emergency room triage. While algorithms often outperform humans in accuracy, they may lack the contextual understanding that humans possess. This highlights the potential for human-AI collaboration, where human input can complement algorithmic predictions to achieve better outcomes. The study focuses on the question of when and how human judgment can improve algorithmic predictions. An example is the diagnosis of atelectasis using chest X-rays, where deep learning models perform well but may benefit from a physician's additional insights. A key idea is the concept of algorithmic indistinguishability, where certain inputs are similar enough that no algorithm can reliably predict outcomes. The authors propose a framework based on multicalibration to identify such subsets and show that human feedback can improve predictions within them. The contributions include a method for incorporating human feedback when it improves predictions, experimental evidence showing human accuracy in specific cases, and analysis of how algorithmic recommendations can be robust to user compliance. The work underscores the value of human-AI complementarity in prediction tasks."
"Summary: The text explores the role of human judgment in enhancing algorithmic predictions, especially in high-stakes scenarios like emergency room triage. It acknowledges that while machine learning models often surpass human experts in accuracy, they may lack contextual understanding that humans can provide. This is exemplified by physicians who can observe patients directly, accessing information not captured in structured data like electronic health records. The text suggests that human-AI collaboration could lead to better outcomes through complementarity, where human input improves algorithmic predictions. Using the example of X-ray classification for atelectasis, it explores whether physicians can offer a “second opinion” by distinguishing between patients with identical imaging data, indicating they possess additional insights. The authors propose a framework for identifying algorithmically indistinguishable inputs—data that no model in a given class can predict effectively—and show how human feedback can enhance predictions within these subsets. This approach is grounded in multicalibration and is adaptable to different model classes. The study presents contributions including a method to integrate human feedback only when beneficial, experimental evidence of human superiority in specific cases, and conditions for algorithmic robustness when recommendations are followed by users. The work highlights the potential for human-AI synergy in improving prediction accuracy and decision-making."
"Summary: The text explores the role of human expertise in enhancing algorithmic prediction, particularly in high-stakes scenarios like emergency room triage. While algorithms often outperform humans in accuracy, they may lack the contextual understanding that humans can provide. For instance, a physician can observe a patient directly, offering insights that imaging data alone cannot capture. The study acknowledges that these two perspectives are not mutually exclusive and suggests that human input can still improve algorithmic predictions, leading to human-AI complementarity. A key question posed is: under what conditions can human judgment enhance algorithmic predictions? The text uses the example of X-ray classification for diagnosing atelectasis to illustrate how physicians might provide additional information not captured in imaging data. A heuristic approach involves determining whether a physician can distinguish between patients with identical imaging data, indicating the presence of unrecorded information. The authors introduce a more general concept of algorithmic indistinguishability, where inputs are considered similar if no algorithm in a specified class can predict outcomes effectively. They propose a framework based on multicalibration to identify these subsets and show that human feedback can improve predictions within them. The contributions include a method for integrating human feedback when it enhances predictions, empirical evidence of human accuracy in specific cases, and conditions for algorithm robustness when recommendations are followed by users."
"Summary: The text explores the role of human judgment in enhancing algorithmic predictions, particularly in high-stakes scenarios like emergency room triage. While algorithms often outperform humans in accuracy, they may miss contextual information that human experts can provide. This is illustrated through the example of X-ray classification for diagnosing atelectasis, where deep learning models perform well but may still benefit from a physician's insights. The paper proposes a framework for identifying algorithmically indistinguishable inputs—cases where no algorithm can predict outcomes accurately. By using human feedback on such cases, predictions can be improved. The approach leverages multicalibration to discover these subsets and demonstrates that human input can surpass algorithmic predictions in specific instances. The work also considers scenarios where algorithms recommend actions to users, and it outlines conditions for robustness against user compliance. The contributions include a method for incorporating human feedback only when it improves predictions, experiments showing human accuracy on specific cases, and a framework for ensuring algorithmic robustness. The study emphasizes the potential for human-AI collaboration to achieve better outcomes than either working alone."
"Summary: The text explores the integration of human judgment with algorithmic prediction, especially in high-stakes scenarios like emergency room triage. While algorithms often outperform humans in accuracy, they may miss contextual information that humans can provide, such as direct patient examination. This suggests that human-AI collaboration could enhance prediction performance through complementarity. The study examines how human feedback can improve algorithmic predictions, particularly in tasks like X-ray classification for diagnosing atelectasis. A key concept is algorithmic indistinguishability—sets of inputs where no algorithm has significant predictive power. The researchers propose a framework using multicalibration to identify such subsets, enabling human input to refine predictions. They demonstrate that even when humans on average underperform algorithms, they can be more accurate on specific, indistinguishable cases. The work also addresses scenarios where algorithms recommend actions to multiple users, and shows how to ensure robustness against varying compliance. Contributions include a method for incorporating human feedback only when beneficial, extending previous results, and validating the framework through experiments. The approach is adaptable to different model classes and prediction tasks, offering a practical and theoretically grounded solution for human-AI collaboration."
